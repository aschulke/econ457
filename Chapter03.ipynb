{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Nonlinear Equations and Complementarity Problems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from numpy import append, array, diagonal, tril, triu\n",
    "from numpy.linalg import inv\n",
    "from scipy.linalg import lu\n",
    "#from scipy.linalg import solve\n",
    "from pprint import pprint\n",
    "from numpy import array, zeros, diag, diagflat, dot\n",
    "\n",
    "from sympy import *\n",
    "import sympy as sym\n",
    "init_printing()\n",
    "\n",
    "import matplotlib as mpl\n",
    "# matplotlib for ploting\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D     # 3d\n",
    "# for inline interactive plotting\n",
    "%matplotlib notebook\n",
    "\n",
    "#mpl.rcParams['savefig.dpi'] = 80\n",
    "mpl.rcParams['figure.dpi'] = 80\n",
    "# from IPython.display import set_matplotlib_formats\n",
    "# set_matplotlib_formats('png', 'pdf')\n",
    "# for better picture quality\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "#https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "#sns.set_style(style= \"whitegrid\")\n",
    "#plt.style.available\n",
    "# bold graph style\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most basic numerical operations encountered in computational economics\n",
    "is to find the solution of a system of nonlinear equations. Nonlinear equations generally\n",
    "arise in one of two forms. In the nonlinear *rootfinding problem*, a function f\n",
    "mapping $R^n$ to $R^n$ is given and one must compute an n-vector $x$, called a *root* of $f$,\n",
    "that satisfies\n",
    "\n",
    "$$f(x) = 0$$\n",
    "\n",
    "\n",
    "In the nonlinear fixed-point problem, a function $g$ from $R^n$ to $R^n$ is given and one\n",
    "must compute an n-vector x called a fixed-point of $g$, that satisfies\n",
    "\n",
    "$$x = g(x)$$\n",
    "\n",
    "\n",
    "The two forms are equivalent. The rootfinding problem may be recast as a fixed-point\n",
    "problem by letting $g(x) = x - f(x)$; conversely, the fixed-point problem may be recast\n",
    "as a rootfinding problem by letting $f(x) = x - g(x)$.\n",
    "\n",
    "\n",
    "In the related complementarity problem, two n-vectors $a$ and $b$, with $a < b$, and\n",
    "a function f from $R^n$ to $R^n$ are given, and one must compute an n-vector $x \\in [a; b]$,\n",
    "that satisfies\n",
    "\n",
    "\n",
    "$$x_i > a_i \\rightarrow f_i(x) \\forall i = 1,...,n$$\n",
    "\n",
    "$$x_i < b_i \\rightarrow f_i(x) \\forall i = 1,...,n$$\n",
    "\n",
    "\n",
    "The rootfinding problem is a special case of complementarity problem in which $a_i =\n",
    "-\\inf$ and $b_i = +\\inf$ for all i. However, the complementarity problem is not simply to\n",
    "find a root that lies within specified bounds. An element $f_i(x)$ may be nonzero at a\n",
    "solution of the complementarity problem, provided that $x_i$ equals one of the bounds\n",
    "$a_i$ or $b_i$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "https://github.com/QuantEcon/QuantEcon.lectures.code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Bisection Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bisection method is perhaps the simplest and most robust method for computing\n",
    "the root of a continuous real-valued function defined on a bounded interval of the real line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bisection method is an iterative procedure. Each iteration begins with an\n",
    "interval known to contain or to bracket a root of f, meaning the function has diffierent\n",
    "signs at the interval endpoints. The interval is bisected into two subintervals of equal\n",
    "length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bisection method's greatest strength is its robustness. In contrast to other\n",
    "rootfinding methods, the bisection method is guaranteed to compute a root to a\n",
    "prescribed tolerance in a known number of iterations, provided valid data are input.\n",
    "\n",
    "\n",
    "https://lectures.quantecon.org/py/scipy.html#roots-and-fixed-points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Bisection_method\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Bisection_method.svg/375px-Bisection_method.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rootfinding\n",
    "plt.figure()\n",
    "x = np.linspace(0,3,100)\n",
    "plt.plot(x,x**3-2);\n",
    "plt.plot(x, x-x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set tolerance \n",
    "tol = 10e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mybisect(f, a, b, tol=10e-10):\n",
    "    \"\"\"\n",
    "    Implements the bisection root finding algorithm, assuming that f is a\n",
    "    real-valued function on [a, b] satisfying f(a) < 0 < f(b).\n",
    "    \"\"\"\n",
    "    lower, upper = a, b\n",
    "\n",
    "    while upper - lower > tol:\n",
    "        middle = 0.5 * (upper + lower)\n",
    "        # === if root is between lower and middle === #\n",
    "        if f(middle) > 0:  \n",
    "            lower, upper = lower, middle\n",
    "        # === if root is between middle and upper  === #\n",
    "        else:              \n",
    "            lower, upper = middle, upper\n",
    "\n",
    "    return 0.5 * (upper + lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f =lambda x: x**3-2\n",
    "#f = lambda x: np.sin(4 * (x - 0.25)) + x + x**20 - 1\n",
    "mybisect(f, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact SciPy provides it’s own bisection function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import bisect\n",
    "## example \n",
    "f =lambda x: x**3-2\n",
    "#f = lambda x: np.sin(4 * (x - 0.25)) + x + x**20 - 1\n",
    "bisect(f, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example of two roots\n",
    "def f(x):\n",
    "    \"\"\"returns f(x)=x^3-2x^2. Has roots at\n",
    "    x=0 (double root) and x=2\"\"\"\n",
    "    return x ** 3 - 2 * x ** 2\n",
    "\n",
    "# main program starts here\n",
    "x = bisect(f, 1.5, 3, xtol=1e-6)\n",
    "\n",
    "print(\"The root x is approximately x=%14.12g,\\n\"\n",
    "      \"the error is less than 1e-6.\" % (x))\n",
    "print(\"The exact error is %g.\" % (2 - x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The `bisect()` method takes three compulsory arguments: \n",
    "\n",
    "- (i) the function `f(x)`, \n",
    "\n",
    "- (ii) a lower limit `a` (for which we have chosen 1.5 in our example) and \n",
    "\n",
    "- (ii) an upper limit `b` (for which we have chosen 3). The optional parameter xtol determines the maximum error of the method.\n",
    "\n",
    "One of the requirements of the bisection method is that the interval `[a, b]` has to be chosen such that the function is either positive at`a` and negative at `b`, or that the function is negative at `a` and postive at `b`. In other words: `a` and `b` have to enclose a root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Function Iteration\n",
    "Function iteration is a relatively simple technique that may be used to compute a\n",
    "fixed-point, $x = g(x)$, of a function from $R^n$ to $R^n$. The technique is also applicable\n",
    "to a rootfinding problem $f(x) = 0$, by recasting it as the equivalent fixed-point\n",
    "problem $x = x - f(x)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "![](http://www.yaroslavvb.com/research/ideas/hmms/ising-fixed-point-blogpost/HTMLFiles/blogpost_6.gif)\n",
    "\n",
    "ref: \n",
    "\n",
    "https://en.wikipedia.org/wiki/Fixed-point_iteration\n",
    "\n",
    "http://yaroslavvb.blogspot.ca/2007/11/belief-propagation-and-fixed-point.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myfixpoint(f, x0, maxit = 1000,tol=10e-10):\n",
    "    # \n",
    "    x = x0\n",
    "    for it in range(maxit):\n",
    "        xold = x\n",
    "        x = f(x) \n",
    "        diff = np.linalg.norm(x - xold)\n",
    "\n",
    "        if diff < tol:\n",
    "            \n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return x**0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#g = lambda x: x**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfixpoint(g, x0=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy has a function for finding (scalar) fixed points too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fixed_point\n",
    "\n",
    "fixed_point(lambda x: x**0.5, 0.4)  # 0.4 is an initial guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Newton's Method\n",
    "\n",
    "\n",
    "In practice, most nonlinear rootfinding problems are solved using *Newton's method*\n",
    "or one of its variants. Newton's method is based on the principle of *successive linearization*. Successive linearization calls for a hard nonlinear problem to be replaced\n",
    "with a sequence of simpler linear problems whose solutions converge to the solution\n",
    "of the nonlinear problem. Newton's method is typically formulated as a rootfinding\n",
    "technique, but may be used to solve a fixed-point problem $x = g(x)$ by recasting it\n",
    "as the rootfinding problem $f(x) = x - g(x) = 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x:f(x)=0\\,.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method starts with a function $f$ defined over the real numbers $x$, the function's derivative $f ′$, and an initial guess $x_0$ for a root of the function $f$. If the function satisfies the assumptions made in the derivation of the formula and the initial guess is close, then a better approximation $x_1$ is\n",
    "\n",
    "$$ x_{1}=x_{0}-{\\frac {f(x_{0})}{f'(x_{0})}}\\,.$$\n",
    "\n",
    "\n",
    "Geometrically, $(x_1, 0)$ is the intersection of the x-axis and the tangent of the graph of f at $(x_0, f (x_0))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is repeated as\n",
    "\n",
    "$$ x_{n+1}=x_{n}-{\\frac {f(x_{n})}{f'(x_{n})}}\\,$$\n",
    "until a sufficiently accurate value is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivation\n",
    "\n",
    "Suppose $f : [a, b] → ℝ$ is a differentiable function defined on the interval $[a, b]$ with values in the real numbers $ℝ$. The formula for converging on the root can be easily derived. Suppose we have some current approximation xn. Then we can derive the formula for a better approximation, $x_{n + 1}$ by referring to the diagram on the right. The equation of the tangent line to the curve $y = f (x)$ at the point $x = x_n$ is\n",
    "\n",
    "\n",
    "\n",
    "$$ y=f'(x_{n})\\,(x-x_{n})+f(x_{n}),$$\n",
    "\n",
    "\n",
    "where $f′$ denotes the derivative of the function $f$.\n",
    "\n",
    "\n",
    "The x-intercept of this line (the value of $x$ such that $y = 0$) is then used as the next approximation to the root, $x_{n+1}$. In other words, setting $y$ to zero and $x$ to $x_{n+1}$ gives\n",
    "\n",
    "$$ 0=f'(x_{n})\\,(x_{n+1}-x_{n})+f(x_{n}).$$\n",
    "Solving for $x_{n+1}$ gives\n",
    "\n",
    "$$ {\\displaystyle x_{n+1}=x_{n}-{\\frac {f(x_{n})}{f'(x_{n})}}.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/NewtonIteration_Ani.gif/450px-NewtonIteration_Ani.gif)\n",
    "\n",
    "ref: \n",
    "https://en.wikipedia.org/wiki/Newton%27s_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SciPy this algorithm is implemented by scipy.optimize.newton\n",
    "\n",
    "Unlike bisection, the Newton-Raphson method uses local slope information\n",
    "\n",
    "This is a double-edged sword:\n",
    "\n",
    "- When the function is well-behaved, the Newton-Raphson method is faster than bisection\n",
    "- When the function is less well-behaved, the Newton-Raphson might fail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import newton\n",
    "\n",
    "f = lambda x: x**3 -2\n",
    "\n",
    "newton(f, 0.2)   # Start the search at initial condition x = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check out how to use newton() in scipy\n",
    "#newton?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton \n",
    "\n",
    "Signature: `newton(func, x0, fprime=None, args=(), tol=1.48e-08, maxiter=50, fprime2=None)`\n",
    "\n",
    "Docstring:\n",
    "Find a zero using the Newton-Raphson or secant method.\n",
    "\n",
    "Find a zero of the function `func` given a nearby starting point `x0`.\n",
    "The Newton-Raphson method is used if the derivative `fprime` of `func`\n",
    "is provided, otherwise the secant method is used.  If the second order\n",
    "derivate `fprime2` of `func` is provided, parabolic Halley's method\n",
    "is used.\n",
    "\n",
    "\n",
    "### Parameters\n",
    "\n",
    "\n",
    "func : function\n",
    "\n",
    "    The function whose zero is wanted. It must be a function of a\n",
    "    **single variable\n",
    "    ** of the form f(x,a,b,c...), where a,b,c... are extra\n",
    "    arguments that can be passed in the `args` parameter.\n",
    "    \n",
    "x0 : float\n",
    "\n",
    "    An initial estimate of the zero that should be somewhere near the\n",
    "    actual zero.\n",
    "    \n",
    "fprime : function, optional\n",
    "\n",
    "    The derivative of the function when available and convenient. If it\n",
    "    is None (default), then the secant method is used.\n",
    "\n",
    "\n",
    "\n",
    "Since `newton` only take one variable function. We use `fsolve` in scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Cournot Duopoly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the use of this function, consider a simple Cournot duopoly model, in which the inverse demand for a good is\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# textbook example \n",
    "q = np.array([1, 1]) # initial point/guess quantity\n",
    "c = np.array([0.6, 0.8]) # cost vector\n",
    "eta = 1.6 # elasticity \n",
    "e = -1/eta    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textbook example. One update of newton algorithm \n",
    "fval = np.sum(q)**e + e * np.sum(q)**(e-1)*(q) - np.diag(c).dot(q) # function value\n",
    "fjac = e*np.sum(q)**(e-1)*(np.ones([2,2]))+e*np.sum(q)**(e-1)*(np.eye(2))+(e-1)*e*np.sum(q)**(e-2)*(q).dot(np.array([1, 1]))-np.diag(c) # function jacbian\n",
    "q = q - np.linalg.inv(fjac).dot(fval) # newton update solution of quantity\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first step result of objective value, and it should go to zero gradually\n",
    "fval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first step result of jacobian\n",
    "fjac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textbook example\n",
    "# iterations of newton\n",
    "maxit = 1000 # maximum number of iteration\n",
    "tol = 10e-10 # tolerance\n",
    "# start from [0.2 0.2] will break . Limitation of Newton\n",
    "#q = np.array([1, 1])\n",
    "#c = np.array([0.6, 0.8])\n",
    "#eta = 1.6\n",
    "#e = -1/eta\n",
    "\n",
    "for it in np.arange(maxit):\n",
    "\n",
    "    fval = np.sum(q)**e + e * np.sum(q)**(e-1)*(q) - np.diag(c).dot(q)\n",
    "    fjac = e*np.sum(q)**(e-1)*(np.ones([2,2]))+e*np.sum(q)**(e-1)*(np.eye(2))+(e-1)*e*np.sum(q)**(e-2)*(q).dot(np.array([1, 1]))-np.diag(c)\n",
    "    q = q - np.linalg.inv(fjac).dot(fval)\n",
    "    if np.linalg.norm(fval)<tol:\n",
    "        break\n",
    "        \n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fval # should equal to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cournot(q):\n",
    "    \"\"\"\n",
    "    A function of Cournot game\n",
    "    input: q quantity of production\n",
    "    output: \n",
    "        fval: objective/function value,could be vector\n",
    "        fjac: value of Jacobian of function, could be matrix\n",
    "    \"\"\"\n",
    "    c = np.array([0.6, 0.8])\n",
    "    eta = 1.6\n",
    "    e = -1 / eta\n",
    "    fval = np.sum(q)**e + e * np.sum(q)**(e-1)*(q) - np.diag(c).dot(q)\n",
    "    fjac = e*np.sum(q)**(e-1)*(np.ones([2,2]))+e*np.sum(q)**(e-1)*(np.eye(2))+\\\n",
    "            (e-1)*e*np.sum(q)**(e-2)*(q).dot(np.array([1, 1]))-np.diag(c)\n",
    "    return fval, fjac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Newton \n",
    "def mynewton(f, x0, maxit=1000, tol=10e-10 ):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        f: function which output fval and fjac\n",
    "        x0: initial guess of solution\n",
    "        maxit: maximum number of iteration\n",
    "        tol： tolerance\n",
    "    output:\n",
    "        x： solution        \n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    for it in np.arange(maxit):\n",
    "        fval, fjac = f(x)\n",
    "        x = x - np.linalg.inv(fjac).dot(fval)\n",
    "        if np.linalg.norm(fval)<tol:\n",
    "            break\n",
    "    return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mynewton(cournot, x0 = np.array([1, 1])) # converge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without backstepping, it does not converge with x0= np.array([0.2, 0.2]). It is overstepping too far.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mynewton(cournot, x0 = np.array([0.2, 0.2])) # break ? without "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More robust implementation can be found in\n",
    "https://github.com/randall-romero/CompEcon-python/blob/master/compecon/nonlinear.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fsolve function\n",
    "\n",
    "Since scipy.optimize.newton only take one variable function. We use fsolve in scipy.optimize to find the roots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "\n",
    "## simple example \n",
    "def f(x):\n",
    "    return x ** 3 - 2 * x ** 2\n",
    "\n",
    "x = fsolve(f, 3)           # one root is at x=2.0\n",
    "\n",
    "print(\"The root x is approximately x=%21.19g\" % x)\n",
    "print(\"The exact error is %g.\" % (2 - x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "The return value of `fsolve` is a numpy array of length n for a root finding problem with `n` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## example\n",
    "def cournot(q):\n",
    "    \"\"\"only output objective/function value\"\"\"\n",
    "    c = np.array([0.6, 0.8])\n",
    "    eta = 1.6\n",
    "    e = -1/eta\n",
    "    fval = np.sum(q)**e + e * np.sum(q)**(e-1)*(q) - np.diag(c).dot(q)\n",
    "    return fval\n",
    "\n",
    "def cournotjac(q):\n",
    "    \"\"\"only output value of jacobian of function \"\"\"\n",
    "    c = np.array([0.6, 0.8])\n",
    "    eta = 1.6\n",
    "    e = -1/eta\n",
    "    fjac = e*np.sum(q)**(e-1)*(np.ones([2,2]))+e*np.sum(q)**(e-1)*(np.eye(2))+(e-1)*e*np.sum(q)**(e-2)*(q).dot(np.array([1, 1]))-np.diag(c)\n",
    "    return fjac    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = fsolve(func = cournot, x0= np.array([0.2, 0.2]))  # without jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q = fsolve(func = cournot,x0= np.array([0.2, 0.2]), fprime= cournotjac)  # with jacobian, fsolve will use numerical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Quasi-Newton Methods\n",
    "\n",
    "Quasi-Newton methods offer an alternative to Newton's method for solving rootfinding\n",
    "problems. Quasi-Newton methods are based on the same successive linearization\n",
    "principle as Newton's method, except that they replace the Jacobian $f'$ with an\n",
    "estimate that is easier to compute. Quasi-Newton methods are easier to implement\n",
    "and less likely to fail due to programming errors than Newton's method because the\n",
    "analyst need not explicitly code the derivative expressions. Quasi-Newton methods,\n",
    "however, often converge more slowly than Newton's method and additionally require\n",
    "the analyst to supply an initial estimate of the function's Jacobian.\n",
    "\n",
    "\n",
    "\n",
    "The **secant method** is the most widely used univariate quasi-Newton method. The\n",
    "secant method is identical to the univariate Newton method, except that it replaces\n",
    "the derivative of f with a finite-difference approximation constructed from the function\n",
    "values at the two previous iterates:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Secant_method\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Secant_method.svg/450px-Secant_method.svg.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton's method to find zeroes of a function ${\\displaystyle g}$ of multiple variables is given by: $ x_{n+1}=x_{n}-[J_{g}(x_{n})]^{-1}g(x_{n})\\,\\!$ where ${\\displaystyle [J_{g}(x_{n})]^{-1}}$ is the left inverse of the Jacobian matrix $ J_{g}(x_{n})$ of ${\\displaystyle g}$ evaluated for ${\\displaystyle x_{n}}$.\n",
    "\n",
    "Strictly speaking, any method that replaces the exact Jacobian ${\\displaystyle J_{g}(x_{n})}$ with an approximation is a **quasi-Newton method.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f'(x_{n-1}) ~= {\\frac {f(x_{n-1})-f(x_{n-2})} {x_{n-1}-x_{n-2}}   }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "$${\\displaystyle x_{n}=x_{n-1}-f(x_{n-1}){\\frac {x_{n-1}-x_{n-2}}{f(x_{n-1})-f(x_{n-2})}}={\\frac {x_{n-2}f(x_{n-1})-x_{n-1}f(x_{n-2})}{f(x_{n-1})-f(x_{n-2})}}}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### *Broyden's method* \n",
    "\n",
    "is the most popular multivariate generalization of the univariate\n",
    "secant method. Broyden's method generates a sequence of vectors $x^{(k)}$ and matrices\n",
    "$A^{(k)}$ that approximate the root of $f$ and the Jacobian $f'$ at the root, respectively.\n",
    "\n",
    "\n",
    "\n",
    "Broyden's method begins with the analyst supplying a guess $x^{(0)}$ for the root of the\n",
    "function and a guess $A^{(0)}$ for the Jacobian of the function at the root. Often, $A^{(0)}$\n",
    "is set equal to the numerical Jacobian of f at $x^{(0)}$. \n",
    "\n",
    "\n",
    "Alternatively, some analysts use\n",
    "a rescaled identity matrix for $A^{(0)}$, though this typically will require more iterations\n",
    "to obtain a solution than if a numerical Jacobian is computed at the outset. Given\n",
    "$x^{(k)}$ and $A^{(k)}$, one updates the root approximation by solving the linear rootfinding\n",
    "problem obtained by replacing $f$ with its first-order Taylor approximation about $x^{(k)}$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://en.wikipedia.org/wiki/Broyden%27s_method\n",
    "\n",
    "\n",
    "\n",
    "To solve a system of $k$ nonlinear equations\n",
    "\n",
    "$${\\displaystyle \\mathbf {f} (\\mathbf {x} )=\\mathbf {0} ,}$$ \n",
    "\n",
    "\n",
    "where $f$ is a vector-valued function of vector $x$:\n",
    "\n",
    "\n",
    "$${\\displaystyle \\mathbf {x} =(x_{1},x_{2},x_{3},\\dotsc ,x_{k})}$$\n",
    "\n",
    "$${\\displaystyle \\mathbf {f} (\\mathbf {x} )=(f_{1}(x_{1},x_{2},\\dotsc ,x_{k}),f_{2}(x_{1},x_{2},\\dotsc ,x_{k}),\\dotsc ,f_{k}(x_{1},x_{2},\\dotsc ,x_{k}))} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For such problems, Broyden gives a generalization of the one-dimensional Newton's method, replacing the derivative with the Jacobian J. The Jacobian matrix is determined iteratively based on the secant equation in the finite difference approximation:\n",
    "\n",
    "$${\\displaystyle \\mathbf {J} _{n}(\\mathbf {x} _{n}-\\mathbf {x} _{n-1})\\simeq \\mathbf {f} (\\mathbf {x} _{n})-\\mathbf {f} (\\mathbf {x} _{n-1}),} $$\n",
    "\n",
    "where n is the iteration index. For clarity, let us define:\n",
    "\n",
    "$${\\displaystyle \\mathbf {f} _{n}=\\mathbf {f} (\\mathbf {x} _{n}),} $$\n",
    "$${\\displaystyle \\Delta \\mathbf {x} _{n}=\\mathbf {x} _{n}-\\mathbf {x} _{n-1},} $$\n",
    "$${\\displaystyle \\Delta \\mathbf {f} _{n}=\\mathbf {f} _{n}-\\mathbf {f} _{n-1},}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so the above may be rewritten as:\n",
    "\n",
    "$${\\displaystyle \\mathbf {J} _{n}\\Delta \\mathbf {x} _{n}\\simeq \\Delta \\mathbf {f} _{n}.} $$\n",
    "\n",
    "\n",
    "The above equation is underdetermined when $k$ is greater than one. \n",
    "\n",
    "Broyden suggests using the current estimate of the Jacobian matrix $\\mathbf {J} _{n-1}$ and improving upon it by taking the solution to the secant equation that is a minimal modification to $\\mathbf {J} _{n-1}$:\n",
    "\n",
    "$${\\displaystyle \\mathbf {J} _{n}=\\mathbf {J} _{n-1}+{\\frac {\\Delta \\mathbf {f} _{n}-\\mathbf {J} _{n-1}\\Delta \\mathbf {x} _{n}}{\\|\\Delta \\mathbf {x} _{n}\\|^{2}}}\\Delta \\mathbf {x} _{n}^{\\mathrm {T} }} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This minimizes the following Frobenius norm:\n",
    "\n",
    "$${\\displaystyle \\|\\mathbf {J} _{n}-\\mathbf {J} _{n-1}\\|_{\\mathrm {f} }.} $$\n",
    "We may then proceed in the Newton direction:\n",
    "\n",
    "$${\\displaystyle \\mathbf {x} _{n+1}=\\mathbf {x} _{n}-\\mathbf {J} _{n}^{-1}\\mathbf {f} (\\mathbf {x} _{n}).}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broyden also suggested using the Sherman-Morrison formula to update directly the inverse of the Jacobian matrix:\n",
    "\n",
    "\n",
    "$${\\displaystyle \\mathbf {J} _{n}^{-1}=\\mathbf {J} _{n-1}^{-1}+{\\frac {\\Delta \\mathbf {x} _{n}-\\mathbf {J} _{n-1}^{-1}\\Delta \\mathbf {f} _{n}}{\\Delta \\mathbf {x} _{n}^{\\mathrm {T} }\\mathbf {J} _{n-1}^{-1}\\Delta \\mathbf {f} _{n}}}\\Delta \\mathbf {x} _{n}^{\\mathrm {T} }\\mathbf {J} _{n-1}^{-1}} $$\n",
    "\n",
    "\n",
    "This first method is commonly known as the **\"good Broyden's method\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, Broyden's method converges if $f$ is continuously differentiable, if $x_0$\n",
    "is \"sufficiently\" close to a root of $f$ at which $f_0$ is invertible, and if $J_0^{-1}$ are\n",
    "\"sufficiently\" close to the Jacobian or inverse Jacobian of $f$ at that root.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Newton's method, the robustness of Broyden's method depends on the regularity of\n",
    "f and its derivatives. Broyden's method may also have difficulty computing a precise\n",
    "root estimate if $f$ is ill-conditioned near the root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/randall-romero/CompEcon-python/blob/master/compecon/tools.py\n",
    "# The script also computes an initial guess for the inverse Jacobian by inverting the finite difierence derivative computed\n",
    "# using the toolbox function fdjac, which is discussed in Chapter 5 (page 107).\n",
    "\n",
    "from compecon.tools import jacobian\n",
    "\n",
    "# def jacobian(func, x, *args, **kwargs):\n",
    "\n",
    "#     # if type(func(x, *args, **kwargs)) is tuple:\n",
    "#     #     F = lambda x: func(x, *args, **kwargs)[0]\n",
    "#     # else:\n",
    "#     #     F = lambda x: func(x, *args, **kwargs)\n",
    "#     F = lambda z: func(z, *args, **kwargs)\n",
    "\n",
    "#     x = x.flatten()\n",
    "#     dx = x.size\n",
    "#     f = F(x)\n",
    "#     df = f.size\n",
    "#     x = x.astype(float)\n",
    "\n",
    "#     ''' Compute Jacobian'''\n",
    "#     tol = np.spacing(1) ** (1/3)\n",
    "\n",
    "#     h = tol * np.maximum(abs(x), 1)\n",
    "#     x_minus_h = x - h\n",
    "#     x_plus_h = x + h\n",
    "#     deltaX = x_plus_h - x_minus_h\n",
    "#     fx = np.zeros((dx, df))\n",
    "\n",
    "#     for k in range(dx):\n",
    "#         xx = x.copy()\n",
    "#         xx[k] = x_plus_h[k]\n",
    "#         fplus = F(xx)\n",
    "\n",
    "#         xx[k] = x_minus_h[k]\n",
    "#         fminus = F(xx)\n",
    "\n",
    "#         fx[k] = np.squeeze((fplus - fminus) / deltaX[k])  # fixme doing this to deal with broadcasting\n",
    "\n",
    "#     return fx.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cournot(q):\n",
    "    \"\"\"only output fval, on jacobian\"\"\"\n",
    "    c = np.array([0.6, 0.8])\n",
    "    eta = 1.6\n",
    "    e = -1 / eta\n",
    "    fval = np.sum(q)**e + e * np.sum(q)**(e-1)*(q) - np.diag(c).dot(q)\n",
    "    return fval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## start iteration \n",
    "# first step\n",
    "f = cournot\n",
    "x0 = np.array([0.2, 0.2]) # initial value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fval = f(x0)\n",
    "fval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using numerical jacobian function to calculate Jacobian\n",
    "fjac = jacobian(cournot,x0 )\n",
    "fjac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using numpy to calculate inversed Jacobian\n",
    "fjacinv = np.linalg.pinv(np.atleast_2d(fjac))\n",
    "fjacinv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop on iteration of broyden method\n",
    "maxit = 1000\n",
    "tol = 10e-10\n",
    "\n",
    "for it in range(maxit):\n",
    "    fnorm = np.linalg.norm(fval)\n",
    "    if fnorm<tol: \n",
    "        break \n",
    "    d = -(fjacinv.dot(fval))\n",
    "    x = x+d\n",
    "    fold = fval\n",
    "    fval = f(x) # update fval\n",
    "    u = fjacinv.dot((fval-fold)) \n",
    "    # update jacobian\n",
    "    fjacinv = fjacinv + np.outer((d-u), np.dot(d.T, fjacinv))/np.dot(d.T,u) ## ? np.outer!!!!! Key\n",
    "x   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear algebra, an outer product is the tensor product of two coordinate vectors, a special case of the Kronecker product of matrices. The outer product of two coordinate vectors ${\\displaystyle \\mathbf {u} } $  and ${\\displaystyle \\mathbf {v} } $ , denoted ${\\displaystyle \\mathbf {u} \\otimes \\mathbf {v} } $, is a matrix ${\\displaystyle \\mathbf {w} } $  such that the coordinates satisfy ${\\displaystyle w_{ij}=u_{i}v_{j}} $. The outer product for general tensors is also called the tensor product.\n",
    "\n",
    "The outer product contrasts with the dot product, which takes as input a pair of coordinate vectors and produces a scalar.\n",
    "\n",
    "(source: https://en.wikipedia.org/wiki/Outer_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without analytic Jacobian, we use Broyden method to find the root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mybroyden(f, x0, maxit = 1000, tol = 10e-10):\n",
    "    \n",
    "    x=x0 #initial guess\n",
    "    A = f(x) # first step of fval\n",
    "    _is_there_jacobian = (type(A) is tuple) and (len(A) == 2) # check if f function output jacobian\n",
    "    \n",
    "    if _is_there_jacobian:\n",
    "        print('Jacobian was  provided by user!')\n",
    "        fval,fjac = f(x)\n",
    "    else:    \n",
    "        print('Jacobian was not provided by user!')\n",
    "        fval = f(x)\n",
    "        try:\n",
    "            fjac = jacobian(f,x) # numerical jacobian\n",
    "        except NameError:\n",
    "            print(\"jacobian function Not in scope!\\n Using identity matrix as jacobian matrix\")\n",
    "            fjac = np.identity(x.size) # if numerical jacobian function does not exist, using identity matrix\n",
    "        else:\n",
    "            print(\"jacobian function In scope!\")\n",
    "            \n",
    "    # using numpy to calculate inversed Jacobian, only for initialize. later, we update it without calculation of new inversed Jacobian.\n",
    "    fjacinv = np.linalg.pinv(np.atleast_2d(fjac))\n",
    "    # or we can use identity matrix as an initiative matrix\n",
    "    #fjacinv = - np.identity(x.size)\n",
    "    for it in range(maxit):\n",
    "        fnorm = np.linalg.norm(fval)\n",
    "        if fnorm<tol: \n",
    "            break \n",
    "        d = -(fjacinv.dot(fval))\n",
    "        # update x\n",
    "        x = x+d\n",
    "        fold = fval\n",
    "        fval = f(x)[0] if _is_there_jacobian else f(x)# two outputs\n",
    "        # update jacobian\n",
    "        u = fjacinv.dot((fval-fold))\n",
    "        fjacinv = fjacinv + np.outer((d-u), np.dot(d.T, fjacinv))/np.dot(d.T,u) ## ? np.outer !!! key\n",
    "    return x   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybroyden(cournot, np.array([0.2, 0.2])) # with a function without jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scipy.optimize.broyden1\n",
    "\n",
    "https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.broyden1.html\n",
    "\n",
    "scipy.optimize.broyden1\n",
    "\n",
    "\n",
    "    scipy.optimize.broyden1(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
    "\n",
    "Find a root of a function, using Broyden’s first Jacobian approximation.\n",
    "\n",
    "This method is also known as “Broyden’s good method”.\n",
    "\n",
    "This algorithm implements the inverse Jacobian Quasi-Newton update\n",
    "\n",
    "![](https://docs.scipy.org/doc/scipy-0.14.0/reference/_images/math/55c80f4be075d374bb3a6683ef676f504969d3a9.png)\n",
    "\n",
    "which corresponds to Broyden’s first Jacobian update\n",
    "\n",
    "![](https://docs.scipy.org/doc/scipy-0.14.0/reference/_images/math/4b7a3d3206120c879b3b7b94a95c2262ffb7b62a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import broyden1\n",
    "broyden1(cournot,np.array([0.2, 0.2]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Problems With Newton Methods\n",
    "\n",
    "\n",
    "\n",
    "Several difficulties commonly arise in the application of Newton and quasi-Newton\n",
    "methods to solving multivariate non-linear equations. The most common cause of\n",
    "failure of Newton-type methods is coding errors committed by the analyst. The next\n",
    "most common cause of failure is the specification of a starting point that is not sufficiently\n",
    "close to a root. And yet another common cause of failure is an ill-conditioned\n",
    "Jacobian at the root. These problems can often be mitigated by appropriate action,\n",
    "though they cannot always be eliminated altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The *first cause of failure*, coding error, may seem obvious and not specific to\n",
    "rootfinding problems. It must be emphasized, however, that with Newton's method,\n",
    "the likelihood of committing an error in coding the analytic Jacobian of the function\n",
    "is often high. A careful analyst can avoid Jacobian coding errors in two ways. First,\n",
    "the analyst could use Broyden's method instead of Newton's method to solve the\n",
    "rootfinding problem. Broyden's method is derivative-free and does not require the\n",
    "explicit coding of the function's analytic Jacobian. Second, the analyst can perform\n",
    "a simple, but highly effiective check of his code by comparing the values computed\n",
    "by his analytic derivatives to those computed using finite diffierence methods. Such a\n",
    "check will almost always detect an error in either the code that returns the function's\n",
    "value or the code that returns its Jacobian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The *second problem, a poor starting value*, can be partially addressed by **\"backstepping\"**.\n",
    "If taking a full Newton (or quasi-Newton) step $x+d$ does not offer an improvement\n",
    "over the current iterate $x$, then one \"backsteps\" toward the current iterate\n",
    "$x$ by repeatedly cutting $d$ in half until $x+d$ does offer an improvement. Whether a step\n",
    "$d$ offers an improvement is measured by the Euclidean norm $||f(x)|| = \\frac{1}{2} f(x)^{T}f(x)$.\n",
    "Clearly, $||f(x)||$ is precisely zero at a root of f, and is positive elsewhere. Thus,\n",
    "one may view an iterate as yielding an improvement over the previous iterate if it\n",
    "reduces the function norm, that is, if $||f(x)|| > ||f(x+ d)||$. Backstepping prevents\n",
    "Newton and quasi-Newton methods from taking a large step in the wrong direction,\n",
    "substantially improving their robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "necessarily prevent Newton type methods\n",
    "from getting stuck at a local minimum of $||f(x)||$. If $||f(x)||$ must decrease with\n",
    "each step, it may be difficult to find a step length that moves away from the current\n",
    "value of x. Most good root-finding algorithms employ so mechanism for getting\n",
    "unstuck. We use a very simple one in which the backsteps continue until either\n",
    "$||f(x)|| > ||f(x+ d)||$ or $ ||f(x+ d/2)|| > ||f(x+ d)||$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cournot(q):\n",
    "    \"\"\"output both function value and jacobian\"\"\"\n",
    "    c = np.array([0.6, 0.8])\n",
    "    eta = 1.6\n",
    "    e = -1 / eta\n",
    "    fval = np.sum(q)**e + e * np.sum(q)**(e-1)*(q) - np.diag(c).dot(q)\n",
    "    fjac = e*np.sum(q)**(e-1)*(np.ones([2,2]))+e*np.sum(q)**(e-1)*(np.eye(2))+\\\n",
    "            (e-1)*e*np.sum(q)**(e-2)*(q).dot(np.array([1, 1]))-np.diag(c)\n",
    "    return fval, fjac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mynewton_backstep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0= np.array([0.2, 0.2]) # initial guess in a hard point\n",
    "f = cournot\n",
    "def mynewton_backstep(f, x0, maxit=1000, tol=1/1000, maxsteps = 1000 ):\n",
    "    \"\"\"\n",
    "    in newton method, using backstep to make sure we don't overshoot\n",
    "    maxstep: maximum number for step checking\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    for it in np.arange(maxit):\n",
    "        fval, fjac = f(x)\n",
    "        fnorm = np.linalg.norm(fval)\n",
    "        if np.linalg.norm(fval)<tol:\n",
    "            break\n",
    "        d =   - np.linalg.inv(fjac).dot(fval)  # step size\n",
    "        fnormold = np.inf # initial fnormold\n",
    "        for backstep in np.arange(maxsteps):\n",
    "            fvalnew = f(x+d)[0] # first output/outcome\n",
    "            fnormnew = np.linalg.norm(fvalnew)\n",
    "            if fnormnew < fnorm: \n",
    "                break # if objective function decrease, improve, so break and continue\n",
    "            if fnormold < fnormnew: \n",
    "                d=2*d # if fnormnew increase,then slow down too much, we speed up a little bit, and go back to previous step before we divide it by 2\n",
    "                break\n",
    "            fnormold = fnormnew;\n",
    "            d = d/2; # if fnormnew greater than fnorm, we divide it by 2, slow down the iteration.  \n",
    "        x = x + d # update x solution with good step size\n",
    "\n",
    "    return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([0.2, 0.2]) # initial value\n",
    "mynewton_backstep(f,x0)  # even start from [0.2,0.2], still converge with backstepping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### even start from [0.2,0.2], still converge with backstepping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Backtracking\n",
    "Newton’s method may not converge for a variety of reasons. One potential problem occurs when the step from $x_k$ to $x_{k+1}$ is so large that the root is stepped over completely. Backtracking is a strategy that combats the problem of overstepping by moving only a fraction of the full step from $x_k$ to $x_{k+1}$.\n",
    "\n",
    "https://github.com/OpenSourceMacro/BootCamp2017/blob/master/Computation/Wk4_DifIntOpt/ACME_Newtons.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check out the scipy newton function\n",
    "#scipy.optimize.newton?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mybroyden_backstep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cournot(q):\n",
    "    \"\"\"only output fval without jacobian since broyden can use numerical jacobian or identity matrix, and update without calcualte new jacobian\"\"\"\n",
    "    c = np.array([0.6, 0.8])\n",
    "    eta = 1.6\n",
    "    e = -1 / eta\n",
    "    fval = np.sum(q)**e + e * np.sum(q)**(e-1)*(q) - np.diag(c).dot(q)\n",
    "    return fval # for Broyden, no need for analytic jacobian\n",
    "\n",
    "x0= np.array([0.2, 0.2])\n",
    "f = cournot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mybroyden_backstep(f, x0, maxit = 1000, tol = 10e-10, maxsteps = 100):\n",
    "    x=x0\n",
    "    fval = f(x)\n",
    "    # using function to calculate Jacobian\n",
    "    fjac = jacobian(f,x)\n",
    "    # using numpy to calculate initial inversed Jacobian\n",
    "    fjacinv = np.linalg.pinv(np.atleast_2d(fjac))\n",
    "    for it in range(maxit):\n",
    "        fnorm = np.linalg.norm(fval)\n",
    "        if fnorm<tol: \n",
    "            break \n",
    "        d = -(fjacinv.dot(fval))\n",
    "        fnormold = np.inf # initial fnormold\n",
    "        for backstep in np.arange(maxsteps):\n",
    "            fvalnew = f(x+d) # first output/outcome\n",
    "            fnormnew = np.linalg.norm(fvalnew)\n",
    "            if fnormnew < fnorm: \n",
    "                break\n",
    "            if fnormold < fnormnew: \n",
    "                d=2*d\n",
    "                break\n",
    "            fnormold = fnormnew;\n",
    "            d = d/2;  # if fnormnew > fnorm, f increase, slow down iteration         \n",
    "        # update x\n",
    "        x = x+d\n",
    "        fold = fval\n",
    "        fval = f(x)\n",
    "        # update jacobian\n",
    "        u = fjacinv.dot((fval-fold))\n",
    "        fjacinv = fjacinv + ((d-u).dot(d.T)*(fjacinv))/(d.T.dot(u)) ## update psudo inversed jacobian withour really calculate new jacobian\n",
    "    return x  \n",
    "\n",
    "mybroyden_backstep(f,x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **third problem, an ill-conditioned Jacobian at the root**, occurs less often, but\n",
    "should not be ignored. An ill-conditioned Jacobian can render inaccurately computed\n",
    "Newton step dx, creating severe difficulties for the convergence of Newton and\n",
    "Newton-type methods. In some cases, ill-conditioning is a structural feature of the\n",
    "underlying model and cannot be eliminated. However, in many cases, ill-conditioning\n",
    "is inadvertently and unnecessarily introduced by the analyst. A common source of\n",
    "avoidable ill-conditioning arises when the natural units of measurements for model\n",
    "variables yield values that vary vastly in order of magnitude. When this occurs, the\n",
    "analyst should consider rescaling the variables so that their values have comparable\n",
    "orders of magnitude, preferably close to unity. Rescaling will generally lead to faster\n",
    "execution time and more accurate results.\n",
    "\n",
    "#### ill-conditioned\n",
    "https://github.com/OpenSourceMacro/BootCamp2017/blob/master/Computation/Wk4_DifIntOpt/ACME_CondStab.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Choosing a Solution Method\n",
    "\n",
    "\n",
    "Numerical analysts have special terms that they use to classify the rates at which\n",
    "iterative routines converge. Specifically, a sequence of iterates x(k) is said to converge\n",
    "to $x^{k}$ at a rate of order p if there is constant $C > 0$ such that\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$ ||x^{(k+1)} - x^*|| <=  C ||x^{(k)} - x^*||^p $$\n",
    "\n",
    "\n",
    "for sufficiently large k. In particular, the rate of convergence is said to be linear if\n",
    "$C < 1$ and $p = 1$, superlinear if $1 < p < 2$, and quadratic if $p = 2$.\n",
    "\n",
    "\n",
    "\n",
    "The asymptotic rates of convergence of the nonlinear equation solution methods\n",
    "discussed earlier are well known. The bisection method converges at a linear rate\n",
    "with $C = 1/2$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Complementarity Problems\n",
    "\n",
    "\n",
    "Many economic models naturally take the form of a complementary problem rather\n",
    "than a rootfinding or fixed point problem. In the complementarity problem, two n-\n",
    "vectors a and b, with a < b, and a function f from $R^n$ to $R^n$  are given, and one must\n",
    "find an n-vector $x \\in [a; b]$, that satisfies\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$x_i > a_i \\rightarrow f_i(x) \\forall i = 1,...,n$$\n",
    "\n",
    "$$x_i < b_i \\rightarrow f_i(x) \\forall i = 1,...,n$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The complementarity conditions require that $f_i(x) = 0$ whenever $a_i < x_i < b_i$. The\n",
    "complementarity problem thus includes the rootfinding problem as a special case in\n",
    "which $a_i = - \\inf$ and $bi = + \\inf$ for all $i$. The complementarity problem, however, is not\n",
    "to find a root that lies within specified bounds. An element $f_i(x)$ may be nonzero at\n",
    "a solution of a complementarity problem, though only if $x_i$ equals one of its bounds.\n",
    "For the sake of brevity, we denote the complementarity problem $CP(f; a; b)$.\n",
    "\n",
    "\n",
    "Complementarity problems arise naturally in economic equilibrium models. In\n",
    "this context, $x$ is an n-vector that represents the levels of certain economic activities.\n",
    "For each $i = 1; 2; : : : ; n$, $a_i$ denotes a lower bound on activity $i$, $b_i$ denotes an upper\n",
    "bound on activity $i$, and $f_i(x)$ denotes the marginal arbitrage profit associated with\n",
    "activity $i$. Disequilibrium arbitrage profit opportunities exist if either $x_i < b_i$ and\n",
    "$f_i(x) > 0$, in which case an incentive exists to increase $x_i$, or $x_i > a_i$ and $f_i(x) < 0$, in\n",
    "which case an incentive exists to decrease $x_i$. An arbitrage-free economic equilibrium\n",
    "obtains if and only if $x$ solves the complementarity problem $CP(f; a; b)$.\n",
    "\n",
    "\n",
    "Complementarity problems also arise naturally in economic optimization models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Complementarity Methods\n",
    "\n",
    "\n",
    "\n",
    "Although the complementarity problem appears quite diffierent from the ordinary\n",
    "rootfinding problem, it actually can be reformulated as one. In particular, x solves the\n",
    "complementarity problem $CP(f; a; b)$ if and only if it solves the rootfinding problem\n",
    "\n",
    "\n",
    "$$ \\hat{f}(x) =min(max(f(x), a - x), b - x) = 0$$\n",
    "\n",
    "\n",
    "(source, http://www.karenkopecky.net/Teaching/eco613614/Notes_ComplementarityMethods.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "The equivalence, however, can easily be demonstrated graphically for the univariate complementarity problem.\n",
    "\n",
    "Figure 3.7 illustrates minmax rootfinding formulation of the same four univariate complementarity problems examined in Figure 3.6. In all four plots, the curves y = a — x and y = b — x are drawn with narrow dashed lines, the curve y = /(x) is drawn with a narrow solid line, and the curve y = f(x) is drawn with a thick solid line; clearly, in all four figures, / lies between the lines y = x — a and y = x — b and coincides with f inside the lines. In Figure 3.7a, $f(a) < 0$ and the unique solution to the complementarity problem is $x^* = a$, which coincides with the unique root of f; in Figure 3.7b,$ f (b) > 0$ and the unique solution to the complementarity problem is $x^* = b$, which coincides with the unique root of f ; in Figure 3.7c, $f (a) > 0 > f (b)$ and the unique solution to the complementarity problem lies between a and b and coincides with the unique root of f (and f). In Figure 3.7d, f is upwardly sloped and possesses multiple roots, all of which, again, coincide with roots of f .\n",
    "\n",
    "![](https://www.ajjacobson.us/bellman-equation/images/2348_33_7.jpg)\n",
    "\n",
    " f'<0, f(a)>0>f(b)\n",
    " \n",
    " \n",
    "![](https://www.ajjacobson.us/bellman-equation/images/2348_33_8.jpg)\n",
    "\n",
    "f'<0, f(b)>0\n",
    "\n",
    "![](https://www.ajjacobson.us/bellman-equation/images/2348_33_9.jpg)\n",
    "\n",
    "b) f'<0, f(b)>0\n",
    "\n",
    "![](https://www.ajjacobson.us/bellman-equation/images/2348_33_10.jpg)\n",
    "\n",
    "d) f'>0\n",
    "\n",
    "The reformulation of the complementarity problem as a rootfinding problem suggests that it may be solved using standard rootfinding algorithms, such as Newton's method. To implement Newton's method for the minmax rootfinding formulation requires computation of the Jacobian J of f . The ith row of J may be derived directly from the Jacobian J of f:\n",
    "\n",
    "$$\\hat J_i (x) = \\begin{cases}\n",
    " & J_i (x),  \\text{ for } a_i - x_i < f_i(x)<b_i -x_i \\\\ \n",
    " & - I_i, \\,\\;\\; \\:\\:  \\text{otherwise} \n",
    "\\end{cases}$$\n",
    "\n",
    "Here, Ii. is the ith row of the identity matrix.\n",
    "\n",
    "\n",
    "\n",
    "(source https://www.ajjacobson.us/bellman-equation/complementarity-methods.html\n",
    "\n",
    "http://www.codecogs.com/latex/eqneditor.php)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def cournot(q):\n",
    "#     c = np.array([0.6, 0.8])\n",
    "#     eta = 1.6\n",
    "#     e = -1 / eta\n",
    "#     fval = np.sum(q)**e + e * np.sum(q)**(e-1)*(q) - np.diag(c).dot(q)\n",
    "#     fjac = e*np.sum(q)**(e-1)*(np.ones([2,2]))+e*np.sum(q)**(e-1)*(np.eye(2))+\\\n",
    "#             (e-1)*e*np.sum(q)**(e-2)*(q).dot(np.array([1, 1]))-np.diag(c)\n",
    "#     return fval, fjac\n",
    "# x0= np.array([0.2, 0.2])\n",
    "# f = cournot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# user-supplied \n",
    "x0, maxit, tol , maxsteps =np.array([2]), 10000,  10e-10, 100 # x0 = 0 converge to 0, x0 = 2 converge to 2.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a simple function return two value, function/objective and jacobian\n",
    "f =lambda x: (1.01 - (1 - x) ** 2,  2 * (1 - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "a = 0\n",
    "b = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x = np.linspace(-0.5,2.5,100)\n",
    "plt.plot(x,np.fmin(np.fmax(f(x)[0], a-x),b-x))\n",
    "plt.hlines(y = 0, xmin=-0.5, xmax=2.5, colors='r', linestyles='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "x = x0 # x0 = 0 converge to 0, x0 = 2 converge to 2.005\n",
    "for it in np.arange(maxit):\n",
    "    fval,fjac = f(x)\n",
    "    fhatval = np.fmin(np.fmax(fval, a-x),b-x)\n",
    "    fhatjac = -np.identity(x.size)\n",
    "    i = (fval> (a-x)) & (fval<(b-x))\n",
    "    if np.any(i):\n",
    "        fhatjac[i] = fjac[i]\n",
    "    x = x - np.linalg.inv(fhatjac).dot(fhatval)\n",
    "    if np.linalg.norm(fhatval)<tol: \n",
    "        break\n",
    "x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implement in function\n",
    "def myncpsolve(f, a,b,x0,maxit = maxit, tol = tol):\n",
    "    x = x0\n",
    "    for it in np.arange(maxit):\n",
    "        fval,fjac = f(x)\n",
    "        fhatval = np.fmin(np.fmax(fval, a-x),b-x)  ## ncp \n",
    "        fhatjac = -np.identity(x.size)\n",
    "        i = (fval> (a-x)) & (fval<(b-x))\n",
    "        if np.any(i):\n",
    "            fhatjac[i] = fjac[i]\n",
    "        x = x - np.linalg.inv(fhatjac).dot(fhatval)\n",
    "        if np.linalg.norm(fhatval)<tol: \n",
    "            break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.array([1.40])\n",
    "myncpsolve(f, a,b,x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.array([2.5])\n",
    "myncpsolve(f, a,b,x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.array([15])\n",
    "myncpsolve(f, a,b,x0)\n",
    "# overshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using Newton's method to find a root of f will often work well. However, in many cases, the **nondifferentiable kinks** in f create difficulties for Newton's method, undermining its ability to converge rapidly and possibly even causing it to cycle. One way to deal with the kinks is to replace f with a function that has the same roots, but is smoother and therefore less prone to numerical difficulties. One function that has proven very effective for solving the complementarity problem in practical applications is Fischer's function \n",
    "\n",
    "$$\\tilde{f}(x)  =  \\phi^- (\\phi^+(f(x),a - x),b - x)$$\n",
    "\n",
    ", where $$\\phi^{\\pm}(u,v) = u_i + v_i \\pm \\sqrt{u_i^2 + v_i^2}$$\n",
    "\n",
    "In Figures 3.8a and 3.8b, the functions $\\hat{f}$ and $\\tilde{f}$ , respectively, are drawn as thick solid lines for a representative complementarity problem. Clearly, $\\hat{f}$ and $\\tilde{f}$ can differ substantially. What is important for solving the complementarity problem, however, is that $\\hat{f}$ and $\\tilde{f}$ possess the same signs and roots and that  $\\tilde{f}$ is smoother than $\\hat{f}$.\n",
    "\n",
    "\n",
    "a) Minimax Formulation \n",
    "\n",
    "![](https://www.ajjacobson.us/bellman-equation/images/2348_33_11.jpg)\n",
    "\n",
    "b) Semismooth Formulation\n",
    "\n",
    "![](https://www.ajjacobson.us/bellman-equation/images/2348_33_12.jpg)\n",
    "\n",
    "(source https://www.ajjacobson.us/bellman-equation/complementarity-methods.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3.9a displays $\\hat f$ (dashed) and $\\tilde f$ (solid) for the complementarity problem and Figure 3.9b magnifies the plot near the origin, making it clear why the problem is hard. Newton's method starting at any value slightly less than 1 will tend to move toward 0. In order to avoid convergence to this false root, Newton's method must take a sufficiently large step to exit the **region of attraction**. This will not happen with $\\hat f$ because 0 poses an upper bound on the positive Newton step. With $\\tilde f$ , however, the function is smooth at its local maximum near the origin, meaning that the Newton step can be very large.\n",
    "\n",
    "![](https://www.ajjacobson.us/bellman-equation/images/2348_33_13.png)\n",
    "\n",
    "(source https://www.ajjacobson.us/bellman-equation/complementarity-methods.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/randall-romero/CompEcon-python/blob/master/compecon/nonlinear.py\n",
    "\n",
    "def fischer(u, v, plus=True):\n",
    "    \"\"\" Computes Fischer's function\n",
    "    phi^±(u, v) = u + v ± sqrt(u^2 + v^2)\n",
    "    In turn, it is assumed that u and v are functions of x. If the Jacobian of Fischer's\n",
    "    function wrt x is required, then partial derivatives du and dv are required.\n",
    "    Fischer's function is useful to transform a complementarity problem into\n",
    "    a nonlinear root-finding problem.\n",
    "    Args:\n",
    "        u:    first term\n",
    "        v:    second term\n",
    "        plus: if True (default), compute  u + v + sqrt(u^2+v^2), else u + v - sqrt(u^2+v^2)\n",
    "    Returns:\n",
    "        phi:                  if either du or dv is None\n",
    "\n",
    "    References:\n",
    "        Miranda and Fackler 2002 Applied Computational Economics and Finance, pp. 49-50\n",
    "    \"\"\"\n",
    "    s = 1 if plus else -1\n",
    "    sq = np.sqrt(u * u + v * v)\n",
    "    ftildeval = u + v + s * sq\n",
    "    return ftildeval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## a simple function return function value and jacobian\n",
    "f = lambda x: (1.01 - (1 - x) ** 2, 2 * (1 - x))\n",
    "a = 0\n",
    "b = np.inf\n",
    "x0=np.array([2.5])\n",
    "x = x0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Fisher function\n",
    "for it in np.arange(maxit):\n",
    "    fval,fjac = f(x)\n",
    "    \n",
    "    ftildeval = fval\n",
    "    da = a-x\n",
    "    db = b-x\n",
    "    hasLowerBound = np.isfinite(a)\n",
    "    hasUpperBound = np.isfinite(b)\n",
    "    if np.any(hasLowerBound):  # apply the Fischer + transform\n",
    "        ftildeval= fischer(ftildeval, da)\n",
    "    if np.any(hasUpperBound):  # apply the Fischer - transform\n",
    "        ftildeval = fischer(ftildeval, db, plus=False)\n",
    "    \n",
    "    ftildejac = -np.identity(x.size)\n",
    "    i = (fval> (a-x)) & (fval<(b-x))\n",
    "    if np.any(i):\n",
    "        ftildejac[i] = fjac[i]\n",
    "    x = x - np.linalg.inv(ftildejac).dot(ftildeval)\n",
    "    if np.linalg.norm(ftildeval)<tol: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance does not improve than CP ？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myssmooth(f, a,b,x0,maxit = maxit, tol = tol):\n",
    "    \"\"\"\n",
    "    Using Fisher function to smooth \n",
    "    f： returns two outputs, function value and jacobian\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    for it in np.arange(maxit):\n",
    "        fval,fjac = f(x)\n",
    "\n",
    "        ftildeval = fval\n",
    "        da = a-x\n",
    "        db = b-x\n",
    "        hasLowerBound = np.isfinite(a)\n",
    "        hasUpperBound = np.isfinite(b) # only apply Fisher if there is a finite bound  ??\n",
    "        if np.any(hasLowerBound):  # apply the Fischer + transform\n",
    "            ftildeval= fischer(ftildeval, da)\n",
    "        if np.any(hasUpperBound):  # apply the Fischer - transform\n",
    "            ftildeval = fischer(ftildeval, db, plus=False)\n",
    "\n",
    "        ftildejac = -np.identity(x.size)\n",
    "        i = (fval> (a-x)) & (fval<(b-x))\n",
    "        if np.any(i):\n",
    "            ftildejac[i] = fjac[i]\n",
    "        x = x - np.linalg.inv(ftildejac).dot(ftildeval)\n",
    "        if np.linalg.norm(ftildeval)<tol: \n",
    "            break\n",
    "    return x    \n",
    "\n",
    "# source\n",
    "#https://github.com/randall-romero/CompEcon-python/blob/master/compecon/nonlinear.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset the function\n",
    "f = lambda x: (1.01 - (1 - x) ** 2, 2 * (1 - x))\n",
    "a = 0\n",
    "b = 10\n",
    "tol = 10e-10\n",
    "maxit = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.array([1.1]) ##??? it does not improve from fhat\n",
    "myssmooth(f, a,b,x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.array([2.5])\n",
    "myssmooth(f, a,b,x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.array([15])\n",
    "myssmooth(f, a,b,x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "\n",
    "scipy.optimize does not have ncpsolve, but dolo python package provides a ncpsolve.\n",
    "\n",
    "import scipy.optimize as opt\n",
    "\n",
    "https://github.com/EconForge/econforge/wiki/CompEcon\n",
    "\n",
    "\n",
    "https://github.com/EconForge/dolo/blob/master/trash/dolo/numeric/solver.py\n",
    "\n",
    "\n",
    "https://github.com/EconForge/dolo/blob/master/dolo/numeric/optimize/ncpsolve.py\n",
    "\n",
    "\n",
    "\n",
    "http://nbviewer.jupyter.org/github/sbustamante/ComputationalMethods/blob/master/material/one-variable-equations.ipynb\n",
    "\n",
    "\n",
    " - Bisection Method\n",
    "\n",
    " - Fixed-point Iteration\n",
    "\n",
    " - Newton-Raphson Method\n",
    "\n",
    " - Secant Method\n",
    " \n",
    " \n",
    "https://github.com/dingliumath/economics/blob/master/3.2%20-%20Finding%20Roots%2C%20Newton's%20Method.ipynb \n",
    " \n",
    "https://github.com/dingliumath/economics/blob/master/3.3%20-%20Ramsey%20via%20Line%20Search%2C%20Shooting%20Method.ipynb\n",
    "\n",
    "https://github.com/dingliumath/economics/blob/master/3.2%20-%20Finding%20Roots%2C%20Built-in%20Methods.ipynb \n",
    "\n",
    "\n",
    "https://github.com/EconForge/econforge/wiki/CompEcon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
