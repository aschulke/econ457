{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "# Finite-Dimensional Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we examine methods for optimizing a function with respect to a finite\n",
    "number of variables. In the finite-dimensional optimization problem, one is given a\n",
    "real-valued function $f$ defined on $X \\subset R^n$ and asked to find an $x^* \\in X$ such that\n",
    "$f(x^*) \\geq f(x)$ for all $x \\in X$. We denote this problem\n",
    "\n",
    "$$\\max_{x \\in X} f(x)$$\n",
    "\n",
    "and call $f$ the objective function, $X$ the feasible set, and $x^*$, if it exists, a maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a close relationship between the finite-dimensional optimization problems\n",
    "discussed in this chapter and the rootfinding and complementarity problems\n",
    "discussed in the previous chapter. The first-order necessary conditions of an unconstrained\n",
    "problem pose a rootfinding problem; the Karush-Kuhn-Tucker first-order\n",
    "necessary conditions of a constrained optimization problem pose a complementarity\n",
    "problem. The rootfinding and complementarity problems associated with optimization\n",
    "problems are special in that they possess a natural merit function, the objective\n",
    "function itself, which may be used to determine whether iterations are converging on\n",
    "a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the years, numerical analysts have studied finite-dimensional optimization\n",
    "problems extensively and have devised a variety of algorithms for solving them quickly\n",
    "and accurately. We begin our discussion with derivative-free methods, which are useful\n",
    "if the objective function is rough or if its derivatives are expensive to compute.\n",
    "We then turn to Newton-type methods for unconstrained optimization, which employ\n",
    "derivatives or derivative estimates to locate an optimum. Univariate unconstrained\n",
    "optimization methods are of particular interest because many multivariate optimization\n",
    "algorithms use the strategy of first determining a linear direction to move in,\n",
    "and then finding the optimal point in that direction. We conclude with a discussion\n",
    "of how to solve constrained optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Derivative-Free Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was the case with univariate rootfinding, optimization algorithms exist that will\n",
    "place progressively smaller brackets around a local maximum of a univariate function.\n",
    "Such methods are relatively slow, but do not require the evaluation of function\n",
    "derivatives and are guaranteed to find a local optimum to a prescribed tolerance in a\n",
    "known number of steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most widely-used derivative-free method is the **golden search** method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose\n",
    "we wish to find a local maximum of a continuous univariate function $f(x)$ on\n",
    "the interval $[a; b]$. \n",
    "\n",
    "Pick any two numbers in the interior of the interval, say $x_1$ and $x_2$\n",
    "with $x_1 < x_2$. \n",
    "\n",
    "Evaluate the function and replace the original interval with $[a; x2]$ if\n",
    "$f(x_1) > f(x_2)$ or with $[x_1; b]$ if $f(x_2) \\geq f(x_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key issue is how to pick the interior evaluation points. \n",
    "\n",
    "Two simple criteria lead\n",
    "to the most widely-used strategy. \n",
    "\n",
    "First, the length of the new interval should be\n",
    "independent of whether the upper or lower bound is replaced.\n",
    "\n",
    "Second, on successive\n",
    "iterations, one should be able to reuse an interior point from the previous iteration so\n",
    "that only one new function evaluation is performed per iteration.\n",
    "\n",
    "\n",
    "These conditions\n",
    "are uniquely satisfied by selecting $x_i = a + \\alpha_i (b - a)$, where\n",
    "\n",
    "$$\\alpha_1 = \\frac{3-\\sqrt 5}{2}$$\n",
    "\n",
    "$$\\alpha_2 = \\frac{\\sqrt 5 -1}{2}$$\n",
    "\n",
    "The value $\\alpha_2$ is known as the golden ratio, a number dear to the hearts of Greek\n",
    "philosophers and Renaissance artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import append, array, diagonal, tril, triu\n",
    "from numpy.linalg import inv\n",
    "from scipy.linalg import lu\n",
    "#from scipy.linalg import solve\n",
    "from pprint import pprint\n",
    "from numpy import array, zeros, diag, diagflat, dot\n",
    "np.random.seed(123)\n",
    "import warnings\n",
    "\n",
    "from sympy import *\n",
    "import sympy as sym\n",
    "init_printing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxit = 1000\n",
    "tol = 1/10000\n",
    "x0= np.array([0,3])\n",
    "f = lambda x: x * np.cos(x ** 2)\n",
    "\n",
    "a,b = 0,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,3, 100)\n",
    "y = f(x)\n",
    "plt.figure()\n",
    "plt.plot(x,y)\n",
    "plt.scatter( np.array([0.8083,2.5234]), f(np.array([0.8083,2.5234])) , c='r' )\n",
    "plt.title(\"Figure 4.1 Maximization of $x cos(x^2)$ via golden search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1 = (3 - np.sqrt(5)) / 2\n",
    "alpha2 = (np.sqrt(5) - 1) / 2\n",
    "if a > b:\n",
    "    a, b = b, a\n",
    "\n",
    "x1 = a + alpha1 * (b - a)\n",
    "x2 = a + alpha2 * (b - a)\n",
    "\n",
    "f1, f2 = f(x1), f(x2)\n",
    "\n",
    "d = (alpha1 * alpha2)*(b - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while d > tol:\n",
    "    d = d * alpha2\n",
    "    if f2 < f1: # x2 is new upper bound\n",
    "        x2, x1 = x1, x1 - d\n",
    "        f2, f1 = f1, f(x1)\n",
    "    else:  # x1 is new lower bound\n",
    "        x1, x2 = x2, x2 + d\n",
    "        f1, f2 = f2, f(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x1 if f1 > f2 else x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f1>f2:\n",
    "    x = x2\n",
    "else:\n",
    "    x = x1      \n",
    "x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mygolden(f,a, b, maxit = 1000, tol = 1/10000):\n",
    "    alpha1 = (3 - np.sqrt(5)) / 2\n",
    "    alpha2 = (np.sqrt(5) - 1) / 2\n",
    "    if a > b:\n",
    "        a, b = b, a\n",
    "        \n",
    "    x1 = a + alpha1 * (b - a)\n",
    "    x2 = a + alpha2 * (b - a)\n",
    "\n",
    "    f1, f2 = f(x1), f(x2)\n",
    "\n",
    "    d = (alpha1 * alpha2)*(b - a) # initial d\n",
    "    while d > tol:\n",
    "        d = d * alpha2 # alpha2 is the golden ratio\n",
    "        if f2 < f1: # x2 is new upper bound\n",
    "            x2, x1 = x1, x1 - d\n",
    "            f2, f1 = f1, f(x1)\n",
    "        else:  # x1 is new lower bound\n",
    "            x1, x2 = x2, x2 + d\n",
    "            f1, f2 = f2, f(x2)\n",
    "            \n",
    "    if f1>f2:\n",
    "        x = x2\n",
    "    else:\n",
    "        x = x1      \n",
    "    return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mygolden(f, 0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution of this script yields the result $x = 0.8083$. As can be seen in Figure 4.1,\n",
    "this point is a local maximum, but not a global maximum in $[0; 3]$. The golden search\n",
    "method is guaranteed to find the global maximum when the function is concave.\n",
    "However, as the present example makes clear, this need not be true when the optimand\n",
    "is not concave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nelder-Mead algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Another widely-used derivative-free optimization method for multivariate functions\n",
    "is the **Nelder-Mead algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nelder-Mead algorithm is simple, but slow and unreliable. However, if a\n",
    "problem involves only a single optimization or costly function and derivative evaluations,\n",
    "the Nelder-Mead algorithm is worth trying. In many problems an optimization\n",
    "problem that is embedded in a larger problem must be solved repeatedly, with the\n",
    "function parameters perturbed slightly with each iteration. For such problems, which\n",
    "are common is dynamic models, one generally will want to use a method that moves\n",
    "more quickly and reliably to the optimum, given a good starting point.\n",
    "\n",
    "\n",
    "(source: https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method)\n",
    "\n",
    "\n",
    "The Nelder–Mead method or downhill simplex method or amoeba method is a commonly applied numerical method used to find the minimum or maximum of an objective function in a multidimensional space. It is applied to nonlinear optimization problems for which derivatives may not be known. However, the Nelder–Mead technique is a heuristic search method that can converge to non-stationary points[1] on problems that can be solved by alternative methods.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Nelder-Mead_Rosenbrock.gif/640px-Nelder-Mead_Rosenbrock.gif)\n",
    "\n",
    "(source: http://www.scholarpedia.org/article/Nelder-Mead_algorithm)\n",
    "\n",
    "The Nelder-Mead algorithm or simplex search algorithm, originally published in 1965 (Nelder and Mead, 1965), is one of the best known algorithms for multidimensional unconstrained optimization without derivatives. This method should not be confused with Dantzig's simplex method for linear programming, which is completely different, as it solves a linearly constrained linear problem.\n",
    "\n",
    "The basic algorithm is quite simple to understand and very easy to use. For these reasons, it is very popular in many fields of science and technology, especially in chemistry and medicine.\n",
    "\n",
    "The method does not require any derivative information, which makes it suitable for problems with non-smooth functions. It is widely used to solve parameter estimation and similar statistical problems, where the function values are uncertain or subject to noise. It can also be used for problems with discontinuous functions, which occur frequently in statistics and experimental mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/fchollet/nelder-mead/blob/master/nelder_mead.py\n",
    "\n",
    "'''\n",
    "    Pure Python/Numpy implementation of the Nelder-Mead algorithm.\n",
    "    Reference: https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method\n",
    "'''\n",
    "import copy\n",
    "\n",
    "def nelder_mead(f, x_start,\n",
    "                step=0.1, no_improve_thr=10e-6,\n",
    "                no_improv_break=10, max_iter=0,\n",
    "                alpha=1., gamma=2., rho=-0.5, sigma=0.5):\n",
    "    '''\n",
    "    @param f (function): function to optimize, must return a scalar score\n",
    "        and operate over a numpy array of the same dimensions as x_start\n",
    "    @param x_start (numpy array): initial position\n",
    "    @param step (float): look-around radius in initial step\n",
    "    @no_improv_thr,  no_improv_break (float, int): break after no_improv_break iterations with\n",
    "        an improvement lower than no_improv_thr\n",
    "    @max_iter (int): always break after this number of iterations.\n",
    "        Set it to 0 to loop indefinitely.\n",
    "    @alpha, gamma, rho, sigma (floats): parameters of the algorithm\n",
    "        (see Wikipedia page for reference)\n",
    "    return: tuple (best parameter array, best score)\n",
    "    '''\n",
    "    # init\n",
    "    dim = len(x_start)\n",
    "    prev_best = f(x_start)\n",
    "    no_improv = 0\n",
    "    res = [[x_start, prev_best]]\n",
    "\n",
    "    for i in range(dim):\n",
    "        x = copy.copy(x_start)\n",
    "        x[i] = x[i] + step\n",
    "        score = f(x)\n",
    "        res.append([x, score])\n",
    "\n",
    "    # simplex iter\n",
    "    iters = 0\n",
    "    while 1:\n",
    "        # order\n",
    "        res.sort(key=lambda x: x[1])\n",
    "        best = res[0][1]\n",
    "\n",
    "        # break after max_iter\n",
    "        if max_iter and iters >= max_iter:\n",
    "            return res[0]\n",
    "        iters += 1\n",
    "\n",
    "        # break after no_improv_break iterations with no improvement\n",
    "        print('...best so far:', best)\n",
    "\n",
    "        if best < prev_best - no_improve_thr:\n",
    "            no_improv = 0\n",
    "            prev_best = best\n",
    "        else:\n",
    "            no_improv += 1\n",
    "\n",
    "        if no_improv >= no_improv_break:\n",
    "            return res[0]\n",
    "\n",
    "        # centroid\n",
    "        x0 = [0.] * dim\n",
    "        for tup in res[:-1]:\n",
    "            for i, c in enumerate(tup[0]):\n",
    "                x0[i] += c / (len(res)-1)\n",
    "\n",
    "        # reflection\n",
    "        xr = x0 + alpha*(x0 - res[-1][0])\n",
    "        rscore = f(xr)\n",
    "        if res[0][1] <= rscore < res[-2][1]:\n",
    "            del res[-1]\n",
    "            res.append([xr, rscore])\n",
    "            continue\n",
    "\n",
    "        # expansion\n",
    "        if rscore < res[0][1]:\n",
    "            xe = x0 + gamma*(x0 - res[-1][0])\n",
    "            escore = f(xe)\n",
    "            if escore < rscore:\n",
    "                del res[-1]\n",
    "                res.append([xe, escore])\n",
    "                continue\n",
    "            else:\n",
    "                del res[-1]\n",
    "                res.append([xr, rscore])\n",
    "                continue\n",
    "\n",
    "        # contraction\n",
    "        xc = x0 + rho*(x0 - res[-1][0])\n",
    "        cscore = f(xc)\n",
    "        if cscore < res[-1][1]:\n",
    "            del res[-1]\n",
    "            res.append([xc, cscore])\n",
    "            continue\n",
    "\n",
    "        # reduction\n",
    "        x1 = res[0][0]\n",
    "        nres = []\n",
    "        for tup in res:\n",
    "            redx = x1 + sigma*(tup[0] - x1)\n",
    "            score = f(redx)\n",
    "            nres.append([redx, score])\n",
    "        res = nres\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# def f(x):\n",
    "#     return math.sin(x[0]) * math.cos(x[1]) * (1. / (abs(x[2]) + 1))\n",
    "\n",
    "#f(x,y) = x^2 - 4*x + y^2 - y - x*y;\n",
    "# f = lambda x: x[0]**2- 4*x[0] + x[1]**2- x[1] - x[0]*x[1]\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return x[0]**2- 4*x[0] + x[1]**2- x[1] - x[0]*x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nelder_mead(f, np.array([0., 0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://codesachin.wordpress.com/2016/01/16/nelder-mead-optimization/\n",
    "from IPython.display import YouTubeVideo\n",
    "# Evaluates the function:\n",
    "# f(x,y) = x^2 - 4*x + y^2 - y - x*y;\n",
    "YouTubeVideo(\"HUqLxHfxWqU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scipy implementation\n",
    "\n",
    "\n",
    "http://www.scipy-lectures.org/advanced/mathematical_optimization/\n",
    "\n",
    "In scipy, scipy.optimize.fmin() implements the Nelder-Mead approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "optimize.fmin(f, [2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize.minimize(f, [2, 2],method='Nelder-Mead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Newton-Raphson Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "banana  \n",
    "\n",
    "$$f = -100*(x_2-x_1^2)^2-(1-x_1)^2$$, \n",
    "\n",
    "so-called because its contours resemble bananas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x,y:(-100*(y-x**2)**2-(1-x)**2)\n",
    "\n",
    "# def f(x,y):\n",
    "#     # the height function\n",
    "#     return (1 - x / 2 + x**5 + y**3) * np.exp(-x**2 -y**2)\n",
    "\n",
    "n = 256\n",
    "x = np.linspace(-0.25, 1.25, n)\n",
    "y = np.linspace(-0.25, 1.25, n)\n",
    "X,Y = np.meshgrid(x, y) \n",
    "\n",
    "plt.figure()\n",
    "x0,y0 = 0,0\n",
    "# use plt.contourf to filling contours\n",
    "# X, Y and value for (X,Y) point\n",
    "plt.contourf(X, Y, f(X, Y), 38, alpha=.75,cmap='bone')# cmap=plt.cm.hot)\n",
    "\n",
    "# use plt.contour to add contour lines\n",
    "C = plt.contour(X, Y, f(X, Y), 38, colors='black', linewidth=.5)\n",
    "\n",
    "plt.clabel(C, inline=True, fontsize=10)\n",
    "# plt.xticks(())\n",
    "# plt.yticks(())\n",
    "\n",
    "# set dot styles\n",
    "plt.scatter([x0, ], [y0, ], s=50, color='b')\n",
    "\n",
    "\n",
    "\n",
    "plt.xlim(-0.25, 1.25)\n",
    "plt.ylim(-0.25, 1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton-Raphson method for maximizing an objective function uses successive\n",
    "quadratic approximations to the objective in the hope that the maxima of the approximants\n",
    "will converge to the maximum of the objective. The Newton-Raphson\n",
    "method is intimately related to the Newton method for solving rootfinding problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Indeed, the Newton-Raphson method is identical to applying Newton's method to\n",
    "compute the root of the gradient of the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Taylor series of $f(x)$ about the point  $x=x_0 + \\epsilon$ is given by\n",
    "\n",
    "$$f(x_0 + \\epsilon) = f(x_0)+ f'(x_0) \\epsilon +\\frac{1}{2} f''(x_0) \\epsilon^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = f(x^{(k)})+ f'(x^{(k)}) (x-x^{(k)}) + \\frac{1}{2}(x-x^{(k)})^T  f''(x^{(k)}) (x-x^{(k)}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the first order condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f'(x^{(k)})+ f''(x^{(k)}) (x-x^{(k)}) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yields the iteration rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$x^{(k+1)} \\leftarrow x^{(k)} -  [f''(x^{(k)})]^{-1} f'(x^{(k)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, the Newton-Raphson method converges if $f$ is twice continuously difierentiable\n",
    "and if the initial value of x supplied by the analyst is sufficiently close to a\n",
    "local maximum of $f$ at which the **Hessian $f''$** is negative definite. There is, however,\n",
    "no generally practical formula for determining what sufficiently close is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton-Raphson method can be robust to the starting\n",
    "value if $f$ is well behaved, for example, if f is **globally concave**. The Newton-Raphson\n",
    "method, however, can be very sensitive to starting value if the function is not globally\n",
    "concave. Also, in practice, the **Hessian $f''$**  must be well-conditioned at the optimum,\n",
    "otherwise rounding errors in the vicinity of the optimum can make it difficult to\n",
    "compute a precise approximate solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton-Raphson algorithm has numerous drawbacks. \n",
    "\n",
    "First, the algorithm\n",
    "requires computation of both the first and second derivatives of the objective function.\n",
    "\n",
    "\n",
    "Second, the Newton-Raphson algorithm offers no **guarantee** that the objective function\n",
    "value may be increased in the direction of the Newton step. Such a guarantee is\n",
    "available only if the Hessian **Hessian $f''(x^k)$** is **negative definite**; otherwise, one may actually\n",
    "move towards a saddle point of f (if the Hessian is indefinite) or even a minimum (if\n",
    "Hessian is **positive definite**).\n",
    "\n",
    "For this reason, the Newton-Raphson method is rarely\n",
    "used in practice, and then only if the objective function is **globally concave**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Quasi-Newton Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Quasi-Newton methods employ a similar strategy to the Newton-Raphson method,\n",
    "but **replace the Hessian of the objective function (or its inverse) with a negative\n",
    "definite approximation, guaranteeing that function value can be increased in the direction\n",
    "of the Newton step**. \n",
    "\n",
    "The most efficient quasi-Newton algorithms employ an\n",
    "approximation to the inverse Hessian, rather than the Hessian itself, in order to avoid\n",
    "performing a linear solve, and employ updating rules that do **not require second\n",
    "derivative information** to ease the burden of implementation and the cost of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In analogy with the Newton-Raphson method, quasi-Newton methods use a search\n",
    "direction of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$d^{(k)} = -B^{(k)} f'(x^{(k)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $B^{(k)}$ is an approximation to the **inverse Hessian** of f at the kth iterate $x^{(k)}$.\n",
    "The vector $d^{(k)}$ is called the **Newton or quasi-Newton step**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more robust quasi-Newton methods do not necessarily take the full Newton\n",
    "step, but rather shorten it or lengthen it in order to obtain improvement in the\n",
    "objective function. This is accomplished by performing a line-search in which one\n",
    "seeks a **step length $s > 0$** that maximizes or nearly maximizes $f (x^{(k)} + sd^{(k)})$. Given\n",
    "the computed step length $s^{(k)}$, one updates the iterate as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x^{(k+1)}=  x^{(k)} + s^{(k)}  d^{(k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quasi-Newton method differ in how the inverse Hessian approximation Bk is constructed\n",
    "and updated. The simplest quasi-Newton method sets\n",
    "\n",
    "$$B^{(k)} = - I $$, \n",
    "\n",
    "where I is  the identity matrix. This leads to a Newton step that is identical to the gradient of\n",
    "the objective function at the current iterate:\n",
    "\n",
    "\n",
    "$$d^{(k)} = f'(x^{(k)})$$\n",
    "\n",
    "The choice of gradient as a step direction is intuitively appealing because the gradient\n",
    "always points in the direction which, to a first order, promises the greatest increase in\n",
    "f. For this reason, this quasi-Newton method is called the method of *steepest ascent.*'\n",
    "\n",
    "\n",
    "The steepest ascent method is simple to implement, but is numerically *less efiicient*\n",
    "in practice than competing quasi-Newton methods that *incorporate* information regarding\n",
    "the **curvature of the objective function**.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **most widely-used** quasi-Newton methods that employ **curvature information**\n",
    "produce a sequence of inverse Hessian estimates that satisfy two conditions. \n",
    "\n",
    "**First,**\n",
    "given that\n",
    "\n",
    "\n",
    "$$d^{(k)} \\approx f''^{-1}(x^{(k)})( f'(x^{(k)}+ d^{(k)} ) - f'(x^{(k)})  )$$\n",
    "\n",
    "the **inverse Hessian estimate** $A^{(-k)}$ is required to satisfy the so-called **quasi-Newton condition:**\n",
    "\n",
    "$$d^{(k)} = B^{(k)} (x^{(k)})( f'(x^{(k)}+ d^{(k)} ) - f'(x^{(k)})  )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second,** the inverse Hessian estimate $A^{(-k)}$  is required to be both **symmetric and\n",
    "negative-definite**, as must be true of the inverse Hessian at a local maximum. The\n",
    "negative definiteness of the Hessian estimate assures that the objective function value\n",
    "can be inreased in the **direction of the Newton step**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two methods that satisfy the quasi-Newton and negative definiteness conditions\n",
    "are the **Davidson-Fletcher-Powell (DFP)** and **Broyden-Fletcher-Goldfarb-Shano (BFGS)**\n",
    "updating methods. The **DFP** method uses the updating scheme\n",
    "\n",
    "\n",
    "$$B \\leftarrow B + \\frac{d d^T}{d^T u}  - \\frac{B u u^T B}{u^T B u} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "$$d = x^{(k+1)} - x^{(k)}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$u = f'(x^{(k+1)}) - f'(x^{(k)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **BFGS** method uses the update scheme\n",
    "\n",
    "\n",
    "$$B \\leftarrow B + \\frac{1}{d^T u}( w d^T + d w^T  - \\frac{w^T u}{d^T u}) d d^T $$\n",
    "\n",
    "where \n",
    "\n",
    "$$w = d - B u$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BFGS algorithm is generally considered superior to DFP, although there\n",
    "are problems for which DFP outperforms BFGS. However, except for the updating\n",
    "formulae, the two methods are identical, so it is easy to implement both and give\n",
    "users the choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list\n",
    "step_methods = ['none','bhhh','bt','golden']                         \n",
    "search_methods = ['steepest','dfp','bfgs']                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dictionary\n",
    "# step_methods = {'none': _step_none,\n",
    "#                      'bhhh': _step_bhhh,\n",
    "#                      'bt': _step_bt,\n",
    "#                      'golden': _step_golden\n",
    "#                      }\n",
    "# search_methods = {'steepest': _search_steepest,\n",
    "#                        'bfgs': _search_bfgs,\n",
    "#                        'dfp': _search_dfp\n",
    "#                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _search_bfgs(f, ff=None, u=None, d=None):\n",
    "#         ud = np.inner(u, d)\n",
    "#         w = d - B.dot(u)\n",
    "#         wd = np.outer(w, d)\n",
    "#         return  B+ ((wd + wd.T) - (np.inner(u, w) * np.outer(d, d)) / ud) / ud\n",
    "#         # self.reset = False\n",
    "\n",
    "# def _search_dfp(self, ff=None, u=None, d=None):\n",
    "#         ud = np.inner(u, d)\n",
    "#         v = B.dot(u)\n",
    "#         return B+ np.outer(d, d) / ud - np.outer(v, v) / np.inner(u, v)\n",
    "#         #self.reset = False\n",
    "\n",
    "# def _search_steepest(self, ff, u=None, d=None):\n",
    "#     return   -np.identity(k) / np.maximum(abs(fx0), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Find the best step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function optstep is not covered in textbook. Only supporing implementation of qnewton.\n",
    "errcode = False\n",
    "def optstep(stepmeth,func, x0, fx0, g0, d, maxstep = 1000):\n",
    "    \"\"\"find the best step size\"\"\"\n",
    "    # take multiple output of function\n",
    "    A = func(x)\n",
    "    k = x0.shape[0] # number of variables\n",
    "    _is_there_jacobian = (type(A) is tuple) and (len(A) == 2)\n",
    "\n",
    "    if _is_there_jacobian:\n",
    "        #print('Jacobian was provided by user!')\n",
    "        f = lambda z:  func(z)[0]\n",
    "        \n",
    "    # several step search method\n",
    "    def _step_none(f, x0, fx0, d,maxstep):\n",
    "        fx = f(x0 + d)\n",
    "        if fx < fx0:\n",
    "            s = 1\n",
    "            errcode = False\n",
    "            return s, f\n",
    "        else:\n",
    "            return _step_golden(f, x0, fx0, d,maxstep)\n",
    "\n",
    "    def _step_bhhh(f, x0, fx0, g0, d,maxstep):\n",
    "        # Intializations\n",
    "        delta = 0.0001\n",
    "        dg = -np.inner(g0, d)  # directional derivative\n",
    "        tol1 = dg * delta\n",
    "        tol0 = dg * (1 - delta)\n",
    "        s, ds = 1, 1\n",
    "        errcode = False\n",
    "\n",
    "        # Bracket the cone\n",
    "        for it in range(maxstep):\n",
    "            x = x0 + s * d\n",
    "            fs = f(x)\n",
    "            temp = (fx0 - fs) / s\n",
    "            if temp < tol0:\n",
    "                ds *= 2\n",
    "                s += ds\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if (tol0 <= temp) and (temp <=tol1):\n",
    "            return s, fs\n",
    "\n",
    "        ds /= 2\n",
    "        s -= ds\n",
    "        it0 = it + 1\n",
    "\n",
    "        # Then use bisection to get inside it\n",
    "        for it in range(it0, maxstep):\n",
    "            ds /= 2\n",
    "            x = x0 + s * d\n",
    "            fs =  f(x)\n",
    "            temp = (fx0 - fs) / s\n",
    "            if temp > tol1:\n",
    "                s -= ds\n",
    "            elif temp < tol0:\n",
    "                s += ds\n",
    "            else:\n",
    "                return s, fs\n",
    "\n",
    "        # If it has not returned yet, call _step_golden!\n",
    "        return _step_golden(f, x0, fx0, d, maxstep)\n",
    "\n",
    "    def _step_bt(f, x0, fx0, g0, d, maxstep):\n",
    "        delta = 1e-4 # Defines cone of convergence; must be on (0,1/2)\n",
    "        ub = 0.5     # Upper bound on acceptable reduction in s.\n",
    "        lb = 0.1     # Lower bound on acceptable reduction in s.\n",
    "        errcode = 0\n",
    "        dg = -np.inner(d, g0)  # directional derivative\n",
    "        tol1 = delta * dg\n",
    "        tol0 = (1 - delta) * dg\n",
    "\n",
    "        # full step\n",
    "        s = 1\n",
    "        fs = f(x0+d)\n",
    "        if (fx0 - fs)   <= tol1:\n",
    "            return s, fs\n",
    "\n",
    "        # quadratic approximation\n",
    "        s2, fs2 = s, fs\n",
    "        s = -0.5 * dg / (-fs + fx0 - dg)\n",
    "        s = max(s, lb)\n",
    "        fs = f(x0 + s * d)\n",
    "        temp = (-fs + fx0) / s\n",
    "        if (tol0 <= temp) and (temp <= tol1):\n",
    "            return s, fs\n",
    "\n",
    "        # cubic approximation\n",
    "        for it in range(3, maxstep):\n",
    "            temp = (s - s2) * np.array([s * s, s2 * s2])\n",
    "            temp = np.array([- fs + fx0 - dg * s, -fs2 + fx0 - dg * s2]) / temp\n",
    "            a = temp[0] - temp[1]\n",
    "            b = s * temp[1] - s2 * temp[0]\n",
    "            s2 = s\n",
    "            fs2 = fs\n",
    "            if np.all(a == 0):  # quadratic fits exactly\n",
    "                s = -0.5 * dg / b\n",
    "            else:\n",
    "                disc = b * b - 3 * a * dg\n",
    "                if np.all(disc < 0):\n",
    "                    errcode = 2\n",
    "                    return s, fs  # complex root\n",
    "                s = (np.sqrt(disc) - b) / (3 * a)\n",
    "\n",
    "            s = np.maximum(np.minimum(s, ub * s2), lb * s2)  # ensures acceptable step size; cp(f, lb, up)\n",
    "            fs = f(x0 + s * d)\n",
    "            temp = (-fs + fx0) / s\n",
    "            if np.all(tol0 <= temp) and np.all(temp <= tol1):\n",
    "                return s, fs\n",
    "\n",
    "        # If it has not returned yet, call _step_golden instead\n",
    "        return _step_golden(f, x0, fx0, d,maxstep)\n",
    "\n",
    "    def _step_golden(f, x0, fx0, d,maxstep):\n",
    "        alpha1 = (3 - np.sqrt(5)) / 2\n",
    "        alpha2 = (np.sqrt(5) - 1) / 2\n",
    "        tol = 1.e-4\n",
    "        tol *= alpha1*alpha2\n",
    "        s = 1\n",
    "        errcode = True\n",
    "        niter = 0\n",
    "        s0 = 0\n",
    "        it = 0\n",
    "\n",
    "        # Find a bracketing interval\n",
    "        fs = f(x0 + d)\n",
    "        if fx0 >= fs:\n",
    "            lenght = alpha1\n",
    "        else:\n",
    "            for it in range(maxstep):\n",
    "                s *= 2\n",
    "                fl = fs\n",
    "                fs = f(x0 + s*d)\n",
    "                if fs <=fl:\n",
    "                    lenght = alpha1 * (s - s0)\n",
    "                    break\n",
    "                else:\n",
    "                    s0 /= 2\n",
    "\n",
    "            if (it + 1) >= maxstep:\n",
    "                s /= 2\n",
    "                fs = fl\n",
    "                return s, fs\n",
    "\n",
    "        xl = x0 + (s + lenght) * d\n",
    "        xs = x0 + (s - lenght) * d\n",
    "\n",
    "        s -= lenght\n",
    "        lenght *= alpha2  # lenght now measures relative distance between xl and xs\n",
    "\n",
    "        fs = f(xs)\n",
    "        fl = f(xl)\n",
    "\n",
    "        # Golden search to find minimum\n",
    "        while it < maxstep:\n",
    "            it += 1\n",
    "            if fs < fl:\n",
    "                s -= lenght\n",
    "                lenght *= alpha2\n",
    "                xs = xl\n",
    "                xl -= lenght * d\n",
    "                fs = fl\n",
    "                fl = f(xl)\n",
    "            else:\n",
    "                lenght *= alpha2\n",
    "                s += lenght\n",
    "                xl = xs\n",
    "                xs += lenght * d\n",
    "                fl = fs\n",
    "                fs = f(xs)\n",
    "\n",
    "            if lenght < tol:\n",
    "                errcode = False\n",
    "                break\n",
    "        if fl > fs:\n",
    "            fs = fl\n",
    "            s -= lenght\n",
    "        return s, fs\n",
    "    \n",
    "    # return resulted s and fx\n",
    "    if stepmeth == None:\n",
    "        return _step_none(f, x0, fx0, d,maxstep)\n",
    "    elif stepmeth == \"bhhh\":\n",
    "        return _step_bhhh(f, x0, fx0, g0, d,maxstep)\n",
    "    elif stepmeth == \"bt\":\n",
    "        return _step_bt(f, x0, fx0, g0, d,maxstep)\n",
    "    elif stepmeth == \"golden\":\n",
    "        return _step_golden(f, x0, fx0, d,maxstep)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bananas function\n",
    "def f(x):\n",
    "    y = (-100*(x[1]-x[0]**2)**2-(1-x[0])**2)\n",
    "    dy = np.array([2*(1-x[0])+400*(x[1]-x[0]**2)*x[0],  -200*(x[1]-x[0]**2)])\n",
    "    return y,dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script assumes that the user\n",
    "has written a Python routine $f$ that evaluates the function at an arbitrary point and\n",
    "that the user has specified a starting point x, an initial guess for the inverse Hessian\n",
    "A, a convergence tolerance tol, and a limit on the number of iterations maxit. The\n",
    "script uses an auxiliary algorithm optstep to determine the step length (discussed\n",
    "in the next section). The algorithm also offers the user a choice on how to select the\n",
    "search direction, searchmeth (1-steepest ascent, 2-DFP, 3-BFGS).\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#         if self.x0 is None or self.x0[0] is None:\n",
    "#             raise ValueError('Initial value is required to solve a OP, none provided!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_list = list()#  sequence of solutions of x for ploting\n",
    "\n",
    "x0 = np.array([1.,0.]) # initial value for x\n",
    "\n",
    "maxit, maxstep, tol,eps0, eps1,all_x  = 10000, 10000, 1/10000,1.0,1.e-12 ,False # keyword arguments\n",
    "\n",
    "x_list = [x0] # first x\n",
    "searchmeth =2 #  pick a search method.\n",
    "stepmeth = \"bt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x0 # initialize\n",
    "k = x.shape[0] # number of variables\n",
    "eps = np.spacing(1) # epsolin\n",
    "\n",
    "A = f(x) # tuble of multiple outputs from object function\n",
    "\n",
    "_is_there_jacobian = (type(A) is tuple) and (len(A) == 2)\n",
    "\n",
    "# get first fx and g. object value and gradient/hessian value. \n",
    "if _is_there_jacobian:\n",
    "    print('Jacobian was provided by user!')\n",
    "    fx0,g0 = f(x)\n",
    "else:    \n",
    "    print('Jacobian was not provided by user!')\n",
    "    fx0 = f(x)\n",
    "    try:\n",
    "        g0 = jacobian(f,x) # customized jacobian function\n",
    "    except NameError:\n",
    "        print(\"jacobian function Not in scope!\\n Using identity matrix as jacobian matrix\")\n",
    "        g0 = np.identity(k)\n",
    "    else:\n",
    "        print(\"jacobian function In scope!\")\n",
    "\n",
    "B = None # inversed Hessian matrix        \n",
    "        \n",
    "if B is None:\n",
    "    B =  -np.identity(k) / np.maximum(abs(fx0), 1)   # using identity matrix as Hessian\n",
    "    print(\"Hessian is not provide and reset as normailized identity matrix! so steepest ascent\") # steepest ascent\n",
    "    \n",
    "d = -np.dot(B, g0)  # search direction    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, fx = optstep(\"bt\" ,f, x, fx0, g0, d, maxstep)\n",
    "\n",
    "s,fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, fx = optstep(\"golden\" ,f, x0, fx0, g0, d, maxstep)\n",
    "s,fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.linalg.norm(g0) < eps: # similar to np.all(g0<eps)\n",
    "    #break #return x\n",
    "    print(\"g0 is less than eps\")\n",
    "if np.all(g0 < eps): # check conditions\n",
    "    #break #return x\n",
    "    print(\"g0 is less than eps\")\n",
    "print(\"Solving nonlinear equations by using {} search method and {} step method\".format(search_methods[searchmeth-1].capitalize(), stepmeth)) \n",
    "\n",
    "print(\"Start iteration......\")\n",
    "\n",
    "\n",
    "for it in range(maxit):\n",
    "    \n",
    "    \n",
    "    d = -np.dot(B, g0)  # search direction\n",
    "    \n",
    "    if (np.inner(d, g0) / (np.inner(d, d))) < eps1:  # must go uphill\n",
    "        B =  -np.identity(k) / np.maximum(abs(fx0), 1) # otherwise use\n",
    "        d = g0 / np.maximum(np.abs(fx0), 1)  # steepest ascent\n",
    "\n",
    "    s, fx = optstep(\"bt\" ,f, x, fx0, g0, d, maxstep)\n",
    "    \n",
    "    if fx <= fx0:\n",
    "        \n",
    "        warnings.warn('Iterations stuck in qnewton')\n",
    "        # break #x # return x\n",
    "        # reset Hessian and d.\n",
    "        B =  -np.identity(k) / np.maximum(abs(fx0), 1) # otherwise use\n",
    "        d = g0.T / np.maximum(abs(fx0), 1)  # steepest ascent\n",
    "        s, fx = optstep(\"bt\" ,f, x, fx0, g0, d, maxstep)\n",
    "        if errcode:\n",
    "            warnings.warn('Cannot find suitable step in qnewton')\n",
    "            # return x\n",
    "            # reset to 1 and fx0\n",
    "            s, fx =  1, fx0\n",
    "    \n",
    "    \n",
    "    d *= s\n",
    "    x = x + d\n",
    "    \n",
    "    x_list.append(x.copy())\n",
    "\n",
    "    if np.any(np.isnan(x) | np.isinf(x)):\n",
    "        raise ValueError('NaNs or Infs encountered')\n",
    "    \n",
    "    # update fx and g\n",
    "    if _is_there_jacobian:\n",
    "        #print('Jacobian was provided by user!')\n",
    "        fx,g = f(x)\n",
    "    else:    \n",
    "        print('Jacobian was not provided by user!')\n",
    "        fx = f(x)\n",
    "        try:\n",
    "            g = jacobian(f,x)\n",
    "        except NameError:\n",
    "            print(\"jacobian function Not in scope!\\n Using identity matrix as jacobian matrix\")\n",
    "            g = np.identity(k)\n",
    "        else:\n",
    "            print(\"jacobian function In scope!\")\n",
    "    \n",
    "\n",
    "    # Test convergence using Marquardt's criteria and gradient test\n",
    "    if ((fx - fx0) / (abs(fx) + eps0) < tol and\n",
    "            np.all(np.abs(d) / (np.abs(x) + eps0) < tol)) or\\\n",
    "            np.all(np.abs(g) < eps):\n",
    "            print(\"Meet the tol. x: \", x)\n",
    "            break\n",
    "#         #return x\n",
    "\n",
    "#     if np.all( np.abs(d)/(np.abs(x) + eps0)< tol) or np.all(np.abs(g) < eps):\n",
    "#         print(\"Meet the tol. x: \", x)\n",
    "#         break\n",
    "        \n",
    "\n",
    "    # Update inverse Hessian\n",
    "    u = g - g0  # change in Jacobian\n",
    "    ud = np.inner(u, d)\n",
    "    \n",
    "    \n",
    "    #print(\"Please specify one search method: 1:steepest ascen;2: DFP;3:BFGS\")\n",
    "    if np.all(np.abs(ud) < eps):\n",
    "        B =  -np.identity(k) / np.maximum(abs(fx0), 1) # otherwise use\n",
    "    else:\n",
    "        if searchmeth == 1 and np.abs(ud) < eps: # steepest ascent\n",
    "            B =  -np.identity(k) / np.maximum(abs(fx), 1)\n",
    "        elif searchmeth == 2: # DFP\n",
    "            v = B.dot(u)\n",
    "            B += np.outer(d, d) / ud - np.outer(v, v) / np.inner(u, v) \n",
    "        elif searchmeth == 3: # BFGS\n",
    "            w = d - B.dot(u)\n",
    "            wd = np.outer(w, d)\n",
    "            B += ((wd + wd.T) - (np.inner(u, w) * np.outer(d, d)) / ud) / ud\n",
    "#         else:\n",
    "#             print(\"Please specify one search method: 1:steepest ascen;2: DFP;3:BFGS\")\n",
    "\n",
    "    # Update iteration\n",
    "    fx0 = fx\n",
    "    g0 = g\n",
    "    print(\"finish {}th iteration...\".format(it))   \n",
    "    \n",
    "#print(\"x list: \" +  for str(x) in x_list)    \n",
    "if it > maxit:\n",
    "    warnings.warn('Maximum iterations exceeded in qnewton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.,0.]) # initial value for x\n",
    "\n",
    "B = None # inversed Hessian matrix        \n",
    "        \n",
    "if B is None:\n",
    "    B =  -np.identity(k) / np.maximum(abs(fx0), 1)   # using identity matrix as Hessian\n",
    "    print(\"Hessian is not provide and reset as normailized identity matrix! so steepest ascent\") # steepest ascent\n",
    "\n",
    "\n",
    "f,x0,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myqnewton(f, x0, B=None, searchmeth = 3,stepmeth = \"bt\" ,maxit = 10000, maxstep = 10000,tol = 1/100000,\\\n",
    "              eps = np.spacing(1),eps0 =1.0, eps1 = 1.e-12, all_x = False):\n",
    "    '''\n",
    "    maxit, maxstep, tol,eps0, eps1  = 10000, 10000, 1/10000,1.0,1.e-12\n",
    "    f: object function and jacobian\n",
    "    x0: initial value\n",
    "    all_x: if we collect x value for plotting\n",
    "    '''\n",
    "    x = x0\n",
    "    if all_x:\n",
    "        x_list = [x0]\n",
    "        \n",
    "    k = x.shape[0] # number of variables\n",
    "    A = f(x)\n",
    "    _is_there_jacobian = (type(A) is tuple) and (len(A) == 2)\n",
    "\n",
    "    if _is_there_jacobian:\n",
    "        print('Jacobian was provided by user!')\n",
    "        fx0,g0 = f(x)\n",
    "    else:    \n",
    "        print('Jacobian was not provided by user!')\n",
    "        fx0 = f(x)\n",
    "        try:\n",
    "            g0 = jacobian(f,x)\n",
    "        except NameError:\n",
    "            print(\"jacobian function Not in scope!\\n Using identity matrix as jacobian matrix\")\n",
    "            g0 = np.identity(k)\n",
    "        else:\n",
    "            print(\"jacobian function In scope!\")    \n",
    "        \n",
    "    if np.all(np.abs(g0) < eps): # similar to np.all(g0<eps)\n",
    "        print(\"abs(g0)< eps...\")\n",
    "        return x\n",
    "    \n",
    "    print(\"Solving nonlinear equations by using {} search method and {} step method\".format(search_methods[searchmeth-1].capitalize(), stepmeth)) \n",
    "\n",
    "    print(\"Start iteration......\")\n",
    "    \n",
    "    #B = None # inversed Hessian matrix        \n",
    "        \n",
    "    if B is None:\n",
    "        B =  -np.identity(k) / np.maximum(abs(fx0), 1)   # using identity matrix as Hessian\n",
    "        print(\"Hessian is not provide and reset as normailized identity matrix! so steepest ascent\") # steepest ascent\n",
    "\n",
    "\n",
    "    for it in range(maxit):\n",
    "\n",
    "\n",
    "        d = -np.dot(B, g0)  # search direction, initial d\n",
    "        \n",
    "        # https://github.com/randall-romero/CompEcon-python/blob/master/compecon/optimize.py\n",
    "        if (np.inner(d, g0) / (np.inner(d, d))) < eps1:  # must go uphill\n",
    "            B =  -np.identity(k) / np.maximum(abs(fx0), 1) # otherwise use\n",
    "            d = g0 / np.maximum(np.abs(fx0), 1)  # steepest ascent\n",
    "        # optimize search step length\n",
    "        s, fx = optstep(stepmeth ,f, x, fx0, g0, d, maxstep)\n",
    "\n",
    "        if fx <= fx0:\n",
    "\n",
    "            warnings.warn('Iterations stuck in qnewton')\n",
    "            #return x\n",
    "            # reset Hessian and d.\n",
    "            B =  -np.identity(k) / np.maximum(abs(fx0), 1) # otherwise use\n",
    "            d = g0.T / np.maximum(abs(fx0), 1)  # steepest ascent\n",
    "            s, fx = optstep(\"bt\" ,f, x, fx0, g0, d, maxstep)\n",
    "            if errcode:\n",
    "                warnings.warn('Cannot find suitable step in qnewton')\n",
    "                # return x\n",
    "                # reset to 1 and fx0\n",
    "                s, fx =  1, fx0\n",
    "\n",
    "        # update d and x\n",
    "        d *= s\n",
    "        x = x + d\n",
    "        # keep record of x sequence in list\n",
    "        if all_x:\n",
    "            x_list.append(x.copy())\n",
    "\n",
    "        if np.any(np.isnan(x) | np.isinf(x)):\n",
    "            raise ValueError('NaNs or Infs encountered')\n",
    "\n",
    "        #  update fx and g again\n",
    "        if _is_there_jacobian:\n",
    "            #print('Jacobian was provided by user!')\n",
    "            fx,g = f(x)\n",
    "        else:    \n",
    "            print('Jacobian was not provided by user!')\n",
    "            fx = f(x)\n",
    "            try:\n",
    "                g = jacobian(f,x)\n",
    "            except NameError:\n",
    "                print(\"jacobian function Not in scope!\\n Using identity matrix as jacobian matrix\")\n",
    "                g = np.identity(k)\n",
    "            else:\n",
    "                print(\"jacobian function In scope!\")\n",
    "\n",
    "\n",
    "        # Test convergence using Marquardt's criteria and gradient test\n",
    "        if ((fx - fx0) / (abs(fx) + eps0) < tol and\n",
    "                np.all(np.abs(d) / (np.abs(x) + eps0) < tol)) or\\\n",
    "                np.all(np.abs(g) < eps):\n",
    "                print(\"Meet the tol. x: \", x)\n",
    "                #break\n",
    "                if all_x:\n",
    "                    return x, x_list\n",
    "                else:\n",
    "                    return x\n",
    "\n",
    "\n",
    "\n",
    "        # Update inverse Hessian\n",
    "        u = g - g0  # change in Jacobian\n",
    "        ud = np.inner(u, d)\n",
    "\n",
    "        # pick a search method\n",
    "        #print(\"Please specify one search method: 1:steepest ascen;2: DFP;3:BFGS\")\n",
    "        if np.all(np.abs(ud) < eps):\n",
    "            B =  -np.identity(k) / np.maximum(abs(fx0), 1) # otherwise use\n",
    "        else:\n",
    "            if searchmeth == 1 and np.abs(ud) < eps: # steepest ascent\n",
    "                B =  -np.identity(k) / np.maximum(abs(fx), 1)\n",
    "            elif searchmeth == 2: # DFP\n",
    "                v = B.dot(u)\n",
    "                B += np.outer(d, d) / ud - np.outer(v, v) / np.inner(u, v) \n",
    "            elif searchmeth == 3: # BFGS\n",
    "                w = d - B.dot(u)\n",
    "                wd = np.outer(w, d)\n",
    "                B += ((wd + wd.T) - (np.inner(u, w) * np.outer(d, d)) / ud) / ud\n",
    "\n",
    "\n",
    "        # Update iteration\n",
    "        fx0 = fx\n",
    "        g0 = g\n",
    "        print(\"finish {}th iteration...\".format(it))   \n",
    "\n",
    "    # end of iteration if exceed the maxit\n",
    "    if it >= maxit:\n",
    "        warnings.warn('Maximum iterations exceeded in qnewton')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.,0.]) # initial value for x\n",
    "B = None # inversed Hessian matrix   \n",
    "myqnewton(f, x0, B, searchmeth = 3,stepmeth = \"bt\" ,maxit = 10000, maxstep = 10000,tol = 1/100000,\\\n",
    "              eps = np.spacing(1),eps0 =1.0, eps1 = 1.e-12, all_x = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.,0.]) # initial value for x\n",
    "B = None # inversed Hessian matrix    \n",
    "myqnewton(f, x0, B, searchmeth = 2,stepmeth = \"bt\" ,maxit = 10000, maxstep = 10000,tol = 1/100000,\\\n",
    "              eps = np.spacing(1),eps0 =1.0, eps1 = 1.e-12, all_x = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.,0.]) # initial value for x\n",
    "myqnewton(f, x0,B = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.,0.]) # initial value for x\n",
    "B = None # inversed Hessian matrix   \n",
    "myqnewton(f, x0, B, searchmeth = 2,stepmeth = \"bt\" ,maxit = 10000, maxstep = 10000,tol = 1/100000,\\\n",
    "              eps = np.spacing(1),eps0 =1.0, eps1 = 1.e-12, all_x = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.,0.]) # initial value for x\n",
    "B = None # inversed Hessian matrix   \n",
    "myqnewton(f, x0, B, searchmeth =1,stepmeth = \"bt\" ,maxit = 10000, maxstep = 10000,tol = 1/100000,\\\n",
    "              eps = np.spacing(1),eps0 =1.0, eps1 = 1.e-12, all_x = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.4 Line Search Methods\n",
    "Just as was the case with rootfinding problems, it is not always best to take a full\n",
    "Newton step. In fact, it may be better to either stop short or move past the Newton\n",
    "step. If we view the Newton step as defining a *search direction*, performing a onedimensional\n",
    "search in that direction will generally produce improved results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "http://reference.wolfram.com/language/tutorial/UnconstrainedOptimizationLineSearchMethods.html\n",
    "\n",
    "https://en.wikipedia.org/wiki/Line_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of diffierent line\n",
    "search methods are used in practice, including the golden search method. \n",
    "\n",
    "The **golden\n",
    "search** algorithm is very reliable, but computationally inefficient. Two alternative\n",
    "schemes are typically used in practice to perform line searches. \n",
    "\n",
    "The first, known as\n",
    "the **Armijo search**, is similar to the backstepping algorithm used in rootfinding and\n",
    "\n",
    "complementarity problems. The idea is to find the minimum power j such that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/smwade/ACME-2/blob/master/line_search/solutions.py\n",
    "\n",
    "def backtracking(f, slope, x, p, a=1, rho=.9, c=10e-4):\n",
    "    \"\"\"Perform a backtracking line search to satisfy the Armijo Conditions.\n",
    "    Parameters:\n",
    "        f (function): the twice-differentiable objective function.\n",
    "        slope (float): The value of grad(f)^T p.\n",
    "        x (ndarray of shape (n,)): The current iterate.\n",
    "        p (ndarray of shape (n,)): The current search direction.\n",
    "        a (float): The intial step length. (set to 1 in Newton and\n",
    "            quasi-Newton methods)\n",
    "        rho (float): A number in (0,1).\n",
    "        c (float): A number in (0,1).\n",
    "    \n",
    "    Returns:\n",
    "        (float) The computed step size satisfying the Armijo condition.\n",
    "    \"\"\"\n",
    "    while f(x + a*p) > f(x) + c * a * slope:\n",
    "        a = float(rho * a)\n",
    "    return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Another widely-used approach, known as **Goldstein search**, is to find any value of\n",
    "s that satisfies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple strategy for locating an acceptable point is to first find a point in or\n",
    "above the cone using step doubling (doubling the value of s at each iteration). If a\n",
    "point above the cone is found first, we have a bracket within which points in the cone\n",
    "must lie. We can then narrow the bracket using the golden search method. We call this the bhhhstep approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach, stepbt, checks to see if s = 1 is in the cone and, if so, maximizes\n",
    "a quadratic approximation to the objective function in the Newton direction\n",
    "constructed from knowledge of f(x), f0(x)d and f(x + d). If the computed step s is\n",
    "acceptable, it is taken. Otherwise, the algorithm iterates until an acceptable step is\n",
    "found using a cubic approximation to the objective function in the Newton direction\n",
    "constructed from knowledge of f(x), f0(x)d, f(x + s(j􀀀1)d) and f(x + s(j)d). stepbt\n",
    "is fast and generally gives good results. It is recommended as the default lines search\n",
    "procedure for general maximization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  4.5 Special Cases\n",
    "Two special cases arise often enough in economic practice (especially in econometrics)\n",
    "to warrant additional discussion. Nonlinear least squares and the maximum likelihood\n",
    "problems have objective functions with special structures that give rise to their\n",
    "own special quasi-Newton methods. The special methods differ from other Newton\n",
    "and quasi-Newton methods only in the choice of the matrix used to approximate the\n",
    "Hessian. Because these problems generally arise in the context of statistical applications,\n",
    "we alter our notation to conform with the conventions for those applications.\n",
    "The optimization takes place with respect to a k-dimensional parameter vector $\\theta$ and\n",
    "n will refer to the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.6 Scipy Optimisation\n",
    "\n",
    "Often we need to find the maximum or minimum of a particular function f(x) where f is a scalar function but x could be a vector. Typical applications are the minimisation of entities such as cost, risk and error, or the maximisation of productivity, efficiency and profit. Optimisation routines typically provide a method to minimise a given function: if we need to maximise f(x) we create a new function g(x) that reverses the sign of f, i.e. g(x)= − f(x) and we minimise g(x).\n",
    "\n",
    "Below, we provide an example showing (i) the definition of the test function and (ii) the call of the `scipy.optimize.fmin` function which takes as argument a function f to minimise and an initial value x0 from which to start the search for the minimum, and which returns the value of x for which f(x) is (locally) minimised. Typically, the search for the minimum is a local search, i.e. the algorithm follows the local gradient. We repeat the search for the minimum for two values (x0 = 1.0 and x0 = 2.0, respectively) to demonstrate that depending on the starting value we may find different minimar of the function f.\n",
    "\n",
    "The majority of the commands (after the two calls to `fmin`) creates the plot of the function, the start points for the searches and the minima obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import arange, cos, exp\n",
    "from scipy.optimize import fmin\n",
    "\n",
    "def f(x):\n",
    "    return cos(x) - 3 * exp( -(x - 0.2) ** 2)\n",
    "\n",
    "# find minima of f(x),\n",
    "# starting from 1.0 and 2.0 respectively\n",
    "minimum1 = fmin(f, 1.0)\n",
    "print(\"Start search at x=1., minimum is\", minimum1)\n",
    "minimum2 = fmin(f, 2.0)\n",
    "print(\"Start search at x=2., minimum is\", minimum2)\n",
    "\n",
    "# plot function\n",
    "plt.figure()\n",
    "x = arange(-10, 10, 0.1)\n",
    "y = f(x)\n",
    "plt.plot(x, y, label='$\\cos(x)-3e^{-(x-0.2)^2}$')\n",
    "plt.xlabel('x')\n",
    "plt.grid()\n",
    "plt.axis([-5, 5, -2.2, 0.5])\n",
    "\n",
    "# add minimum1 to plot\n",
    "plt.plot(minimum1, f(minimum1), 'vr',\n",
    "           label='minimum 1')\n",
    "# add start1 to plot\n",
    "plt.plot(1.0, f(1.0), 'or', label='start 1')\n",
    "\n",
    "# add minimum2 to plot\n",
    "plt.plot(minimum2,f(minimum2),'vg',\\\n",
    "           label='minimum 2')\n",
    "# add start2 to plot\n",
    "plt.plot(2.0,f(2.0),'og',label='start 2')\n",
    "\n",
    "plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reference\n",
    "\n",
    "\n",
    "- Optimization and Solving Systems of Equations in Julia\n",
    "\n",
    "https://github.com/pkofod/JC2017\n",
    "\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=E_UlaGoObTw\n",
    "\n",
    "\n",
    "Using optimization routines from scipy and statsmodels\n",
    "http://people.duke.edu/~ccc14/sta-663-2017/14C_Optimization_In_Python.html\n",
    "http://people.duke.edu/~ccc14/sta-663-2017/14B_Multivariate_Optimization.html#convexity\n",
    "\n",
    "\n",
    "http://people.duke.edu/~ccc14/sta-663-2017/14B_Multivariate_Optimization.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
